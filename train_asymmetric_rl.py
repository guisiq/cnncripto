"""
Pipeline de Treinamento AssimÃ©trico (RL)

EstratÃ©gia:
- MacroNet treina com dados longos (492 candles = 41h) â†’ Atualiza 1x
- MicroNet treina com dados curtos (60 candles = 5h) â†’ Atualiza 2x
- Ratio: 1 update MacroNet : 2 updates MicroNet

Vantagens:
1. MacroNet captura tendÃªncias de longo prazo (estÃ¡vel)
2. MicroNet adapta-se rÃ¡pido a mudanÃ§as (Ã¡gil)
3. EficiÃªncia computacional (MacroNet mais pesada)
4. SeparaÃ§Ã£o: EstratÃ©gia (macro) vs TÃ¡tica (micro)
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt

from src.pipeline import TradingPipeline
from src.config import config
from src.logger import get_logger

logger = get_logger(__name__)


class AsymmetricPolicyNetwork(nn.Module):
    """
    Rede de polÃ­tica com componentes separados:
    - Macro: DEEP Encoder-Decoder com convoluÃ§Ã£o (30+ camadas)
    - Micro: Deep MLP sem convoluÃ§Ã£o (10 camadas)
    """
    
    def __init__(
        self,
        macro_features: int,
        micro_features: int,
        macro_embedding_dim: int = 256,  # DOBRADO: 128 â†’ 256 dim
        micro_hidden_dim: int = 256,
        num_actions: int = 3
    ):
        super().__init__()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MACRO ENCODER-DECODER (30+ camadas com convoluÃ§Ã£o)
        # Arquitetura: Encoder (comprime) â†’ Bottleneck â†’ Decoder (expande)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Reshape helper: Linear â†’ reshape para (batch, channels, length)
        self.macro_input_dim = macro_features
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ENCODER (compress: 512 â†’ 256 â†’ 128 â†’ 64 â†’ 32)
        # 15 camadas no encoder
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.macro_encoder = nn.Sequential(
            # Stage 1: Input projection (512)
            nn.Linear(macro_features, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            # Stage 2: Deep processing (512 â†’ 512) - 4 blocos residuais
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            # Stage 3: Compress to 256
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Stage 4: Deep 256 (3 blocos)
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Stage 5: Compress to 128
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # BOTTLENECK (128 â†’ 64 â†’ 32 â†’ 64 â†’ 128)
        # 10 camadas no bottleneck
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.macro_bottleneck = nn.Sequential(
            # Compress deep
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Extreme bottleneck
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            nn.Linear(32, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            # Start expansion
            nn.Linear(32, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Expand to 128
            nn.Linear(64, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # DECODER (expand: 128 â†’ 256 â†’ 512 â†’ macro_embedding_dim)
        # 12 camadas no decoder
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.macro_decoder = nn.Sequential(
            # Stage 1: Deep 128 (2 blocos)
            nn.Linear(128, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(128, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Stage 2: Expand to 256
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Stage 3: Deep 256 (3 blocos)
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Stage 4: Final projection
            nn.Linear(256, macro_embedding_dim),
            nn.LayerNorm(macro_embedding_dim),
            nn.ReLU(),
        )
        
        # TOTAL MacroNet: 15 (encoder) + 10 (bottleneck) + 12 (decoder) = 37 camadas!
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MICRO PROCESSOR (10 camadas densas, sem convoluÃ§Ã£o)
        # Arquitetura: Recebe micro_features + macro_embedding
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Agora macro_embedding (256) tambÃ©m entra na micro
        # micro_features + macro_embedding â†’ micro_hidden_dim
        micro_input_dim = micro_features + macro_embedding_dim  # micro_features + 256
        
        self.micro_processor = nn.Sequential(
            # Layer 1-2: Input processing (combina micro features + macro embedding)
            nn.Linear(micro_input_dim, micro_hidden_dim),
            nn.LayerNorm(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.LayerNorm(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Layer 3-4: Deep processing
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.LayerNorm(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.LayerNorm(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Layer 5-6: Compression start
            nn.Linear(micro_hidden_dim, micro_hidden_dim // 2),
            nn.LayerNorm(micro_hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 2, micro_hidden_dim // 2),
            nn.LayerNorm(micro_hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Layer 7-8: Further compression
            nn.Linear(micro_hidden_dim // 2, micro_hidden_dim // 4),
            nn.LayerNorm(micro_hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 4, micro_hidden_dim // 4),
            nn.LayerNorm(micro_hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Layer 9-10: Final projection
            nn.Linear(micro_hidden_dim // 4, micro_hidden_dim // 4),
            nn.LayerNorm(micro_hidden_dim // 4),
            nn.ReLU(),
        )
        
        # TOTAL MicroNet: 10 camadas
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DECISION HEAD (combina macro + micro)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        micro_output_dim = micro_hidden_dim // 4  # 64 if micro_hidden_dim=256
        combined_dim = macro_embedding_dim + micro_output_dim + 2  # +2 para position e cash
        
        self.decision_head = nn.Sequential(
            nn.Linear(combined_dim, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, num_actions),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, macro_features, micro_features, position, cash_ratio):
        """
        Args:
            macro_features: (batch, macro_feature_dim) - contexto longo
            micro_features: (batch, micro_feature_dim) - contexto curto
            position: (batch,) - posiÃ§Ã£o atual
            cash_ratio: (batch,) - cash / capital
        
        Fluxo:
        1. Macro processa long-term context â†’ macro_emb (256)
        2. Micro recebe micro_features + macro_emb concatenados
        3. Decision_head combina tudo para aÃ§Ã£o final
        """
        # Deep Macro Encoder-Decoder (37 camadas)
        macro_encoded = self.macro_encoder(macro_features)        # 15 camadas
        macro_bottleneck = self.macro_bottleneck(macro_encoded)   # 10 camadas
        macro_emb = self.macro_decoder(macro_bottleneck)          # 12 camadas â†’ [batch, 256]
        
        # Deep Micro Processor recebe micro_features + macro_emb concatenados
        # Macro embedding guia a processamento micro (hierarchical input)
        micro_input = torch.cat([micro_features, macro_emb], dim=-1)  # [batch, micro_dim+256]
        micro_feat = self.micro_processor(micro_input)  # 10 camadas
        
        # Combine all
        combined = torch.cat([
            macro_emb,
            micro_feat,
            position.unsqueeze(-1),
            cash_ratio.unsqueeze(-1)
        ], dim=-1)
        
        # Decision
        action_probs = self.decision_head(combined)
        
        return action_probs, macro_emb


class TradingEnvironmentRL:
    """Ambiente de trading simplificado para RL"""
    
    def __init__(
        self,
        prices: np.ndarray,
        macro_features: np.ndarray,
        micro_features: np.ndarray,
        initial_capital: float = 10000.0,
        commission: float = 0.001
    ):
        self.prices = prices
        self.macro_features = macro_features
        self.micro_features = micro_features
        self.initial_capital = initial_capital
        self.commission = commission
        
        self.reset()
    
    def reset(self):
        """Reset environment"""
        self.step_idx = 0
        self.cash = self.initial_capital
        self.position = 0.0
        self.portfolio_value = self.initial_capital
        
        return self._get_state()
    
    def _get_state(self):
        """Get current state"""
        if self.step_idx >= len(self.prices):
            return None
        
        return {
            'macro_features': self.macro_features[self.step_idx],
            'micro_features': self.micro_features[self.step_idx],
            'price': self.prices[self.step_idx],
            'position': self.position,
            'cash': self.cash,
            'portfolio_value': self.portfolio_value
        }
    
    def step(self, action: int) -> Tuple[dict, float, bool]:
        """
        Execute action: 0=HOLD, 1=BUY, 2=SELL
        
        Returns:
            next_state, reward, done
        """
        if self.step_idx >= len(self.prices) - 1:
            return None, 0.0, True
        
        current_price = self.prices[self.step_idx]
        next_price = self.prices[self.step_idx + 1]
        
        # Execute action
        if action == 1 and self.position < 1.0:  # BUY
            trade_size = 1.0 - self.position
            cost = trade_size * current_price * (1 + self.commission)
            if cost <= self.cash:
                self.cash -= cost
                self.position += trade_size
        
        elif action == 2 and self.position > -1.0:  # SELL
            trade_size = self.position - (-1.0)
            proceeds = trade_size * current_price * (1 - self.commission)
            self.cash += proceeds
            self.position -= trade_size
        
        # Advance
        self.step_idx += 1
        
        # Calculate reward (P&L)
        price_change = next_price - current_price
        position_pnl = self.position * price_change
        reward = (position_pnl / self.initial_capital) * 100
        
        # Penalty for trading
        if action != 0:
            reward -= 0.01
        
        # Update portfolio
        self.portfolio_value = self.cash + (self.position * next_price)
        
        done = self.step_idx >= len(self.prices) - 1
        next_state = self._get_state()
        
        return next_state, reward, done


class AsymmetricRLTrainer:
    """Treinador com updates assimÃ©tricos"""
    
    def __init__(
        self,
        macro_features_dim: int,
        micro_features_dim: int,
        learning_rate_macro: float = 0.0001,  # Menor para macro (estÃ¡vel)
        learning_rate_micro: float = 0.0005,  # Maior para micro (Ã¡gil)
        gamma: float = 0.99,
        device: str = "cpu"
    ):
        # ForÃ§a MPS quando disponÃ­vel em Apple Silicon, senÃ£o usa device passado
        if torch.backends.mps.is_available():
            effective_device = "mps"
            print(f"âœ… MPS (Apple Silicon GPU) disponÃ­vel - usando MPS")
        else:
            effective_device = device
            print(f"âš ï¸  MPS nÃ£o disponÃ­vel - usando device: {device}")
        
        self.device = torch.device(effective_device)
        self.gamma = gamma
        
        # Build policy network
        self.policy = AsymmetricPolicyNetwork(
            macro_features=macro_features_dim,
            micro_features=micro_features_dim,
            macro_embedding_dim=256,  # DOBRADO: 128 â†’ 256 dim
            micro_hidden_dim=256,      # TambÃ©m aumentado para 256
            num_actions=3
        ).to(self.device)
        
        # Optimizers separados para controle fino
        self.optimizer_macro = optim.Adam(
            list(self.policy.macro_encoder.parameters()) +
            list(self.policy.macro_bottleneck.parameters()) +
            list(self.policy.macro_decoder.parameters()),
            lr=learning_rate_macro
        )
        
        self.optimizer_micro = optim.Adam(
            list(self.policy.micro_processor.parameters()) +
            list(self.policy.decision_head.parameters()),
            lr=learning_rate_micro
        )
        
        # HistÃ³rico
        self.history = {
            'episode': [],
            'total_reward': [],
            'portfolio_value': [],
            'macro_updates': [],
            'micro_updates': []
        }
        
        self.macro_update_count = 0
        self.micro_update_count = 0
        
        logger.info(
            "asymmetric_trainer_initialized",
            macro_lr=learning_rate_macro,
            micro_lr=learning_rate_micro
        )
    
    def select_action(self, state: dict) -> Tuple[int, torch.Tensor]:
        """Select action using policy"""
        macro_feat = torch.FloatTensor(state['macro_features']).unsqueeze(0).to(self.device)
        micro_feat = torch.FloatTensor(state['micro_features']).unsqueeze(0).to(self.device)
        position = torch.FloatTensor([state['position']]).to(self.device)
        cash_ratio = torch.FloatTensor([state['cash'] / 10000.0]).to(self.device)
        
        action_probs, _ = self.policy(macro_feat, micro_feat, position, cash_ratio)
        
        dist = Categorical(action_probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob
    
    def train_episode(
        self,
        env: TradingEnvironmentRL,
        update_macro: bool,
        update_micro: bool
    ) -> Tuple[float, float]:
        """
        Train one episode with selective updates
        
        Args:
            update_macro: Se deve atualizar MacroNet neste episÃ³dio
            update_micro: Se deve atualizar MicroNet neste episÃ³dio
        
        Returns:
            total_reward, final_portfolio
        """
        state = env.reset()
        
        log_probs = []
        rewards = []
        
        # Collect trajectory
        while True:
            action, log_prob = self.select_action(state)
            next_state, reward, done = env.step(action)
            
            log_probs.append(log_prob)
            rewards.append(reward)
            
            if done:
                break
            
            state = next_state
        
        # Calculate discounted returns
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        
        returns = torch.FloatTensor(returns).to(self.device)
        
        # Normalize
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Policy loss
        policy_loss = []
        for log_prob, G in zip(log_probs, returns):
            policy_loss.append(-log_prob * G)
        
        policy_loss = torch.stack(policy_loss).sum()
        
        # Selective backpropagation
        if update_macro:
            self.optimizer_macro.zero_grad()
            policy_loss.backward(retain_graph=update_micro)
            torch.nn.utils.clip_grad_norm_(
                list(self.policy.macro_encoder.parameters()) +
                list(self.policy.macro_bottleneck.parameters()) +
                list(self.policy.macro_decoder.parameters()),
                1.0
            )
            self.optimizer_macro.step()
            self.macro_update_count += 1
        
        if update_micro:
            if not update_macro:
                self.optimizer_micro.zero_grad()
                policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.policy.micro_processor.parameters()) +
                list(self.policy.decision_head.parameters()),
                1.0
            )
            self.optimizer_micro.step()
            self.micro_update_count += 1
        
        total_reward = sum(rewards)
        
        return total_reward, env.portfolio_value

    def train_batch(
        self,
        envs: List[TradingEnvironmentRL],
        update_macro: bool,
        update_micro: bool,
        max_steps: int = None
    ) -> Tuple[float, float]:
        """
        Vectorized rollout across multiple environments (synchronized timesteps).

        Runs all envs in lock-step, calling the policy once per timestep for the whole batch.
        Returns mean reward and mean final portfolio across envs.
        """
        num_envs = len(envs)

        # Reset all envs and collect initial states
        states = [env.reset() for env in envs]
        done = [False] * num_envs

        log_probs_per_env: List[List[torch.Tensor]] = [[] for _ in range(num_envs)]
        rewards_per_env: List[List[float]] = [[] for _ in range(num_envs)]

        step = 0
        while True:
            # Stop conditions
            if max_steps is not None and step >= max_steps:
                break
            if all(done):
                break

            # Build batch tensors for active envs
            macro_batch = []
            micro_batch = []
            pos_batch = []
            cash_batch = []
            active_idx = []

            for i, s in enumerate(states):
                if done[i] or s is None:
                    continue
                macro_batch.append(np.asarray(s['macro_features'], dtype=np.float32))
                micro_batch.append(np.asarray(s['micro_features'], dtype=np.float32))
                pos_batch.append(s['position'])
                cash_batch.append(s['cash'] / 10000.0)
                active_idx.append(i)

            if len(active_idx) == 0:
                break

            macro_tensor = torch.FloatTensor(np.stack(macro_batch)).to(self.device)
            micro_tensor = torch.FloatTensor(np.stack(micro_batch)).to(self.device)
            pos_tensor = torch.FloatTensor(pos_batch).to(self.device)
            cash_tensor = torch.FloatTensor(cash_batch).to(self.device)

            # Forward once for the batch
            action_probs, _ = self.policy(macro_tensor, micro_tensor, pos_tensor, cash_tensor)
            dists = Categorical(action_probs)
            actions = dists.sample()
            log_probs = dists.log_prob(actions)

            # Step each active env with its action
            for idx_in_batch, env_idx in enumerate(active_idx):
                action = int(actions[idx_in_batch].item())
                lp = log_probs[idx_in_batch]
                next_state, reward, is_done = envs[env_idx].step(action)

                log_probs_per_env[env_idx].append(lp)
                rewards_per_env[env_idx].append(float(reward))
                states[env_idx] = next_state
                done[env_idx] = is_done

            step += 1

        # Compute returns per env and total loss
        total_loss = 0.0
        device = self.device
        for i in range(num_envs):
            rewards = rewards_per_env[i]
            if len(rewards) == 0:
                continue
            returns = []
            G = 0
            for r in reversed(rewards):
                G = r + self.gamma * G
                returns.insert(0, G)

            returns = torch.FloatTensor(returns).to(device)
            if len(returns) > 1:
                returns = (returns - returns.mean()) / (returns.std() + 1e-8)

            l = []
            for lp, G in zip(log_probs_per_env[i], returns):
                l.append(-lp * G)

            if len(l) > 0:
                env_loss = torch.stack(l).sum()
                total_loss = total_loss + env_loss

        if isinstance(total_loss, float):
            # no trainable data
            return 0.0, float(np.mean([e.portfolio_value for e in envs]))

        policy_loss = total_loss

        # Selective backpropagation (same logic as train_episode)
        if update_macro:
            self.optimizer_macro.zero_grad()
            policy_loss.backward(retain_graph=update_micro)
            torch.nn.utils.clip_grad_norm_(
                list(self.policy.macro_encoder.parameters()) +
                list(self.policy.macro_bottleneck.parameters()) +
                list(self.policy.macro_decoder.parameters()),
                1.0
            )
            self.optimizer_macro.step()
            self.macro_update_count += 1

        if update_micro:
            if not update_macro:
                self.optimizer_micro.zero_grad()
                policy_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.policy.micro_processor.parameters()) +
                list(self.policy.decision_head.parameters()),
                1.0
            )
            self.optimizer_micro.step()
            self.micro_update_count += 1

        # Aggregate metrics
        mean_reward = np.mean([np.sum(r) for r in rewards_per_env if len(r) > 0]) if any(len(r) > 0 for r in rewards_per_env) else 0.0
        mean_portfolio = float(np.mean([e.portfolio_value for e in envs]))

        return mean_reward, mean_portfolio


def prepare_asymmetric_data(
    df: pd.DataFrame,
    macro_window: int = 492,  # 41h de contexto
    micro_window: int = 60     # 5h de contexto
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepara dados com janelas assimÃ©tricas
    
    Returns:
        prices, macro_features, micro_features
    """
    print(f"\nğŸ“Š Preparando dados assimÃ©tricos...")
    print(f"   Macro window: {macro_window} candles ({macro_window*5/60:.1f}h)")
    print(f"   Micro window: {micro_window} candles ({micro_window*5/60:.1f}h)")
    
    # Extrair features numÃ©ricas (excluir colunas nÃ£o-numÃ©ricas)
    numeric_cols = []
    for col in df.columns:
        if np.issubdtype(df[col].dtype, np.number):
            numeric_cols.append(col)
    
    features = df[numeric_cols].fillna(0).values.astype(np.float32)
    prices = df['close'].values.astype(np.float32)
    
    # Criar janelas rolantes
    macro_features_list = []
    micro_features_list = []
    prices_list = []
    
    for i in range(macro_window, len(df)):
        # Macro: Ãºltimos 492 candles agregados
        macro_window_data = features[i-macro_window:i]
        macro_agg = np.concatenate([
            macro_window_data.mean(axis=0),
            macro_window_data.std(axis=0),
            macro_window_data[-1]  # Ãºltimo valor
        ])
        
        # Micro: Ãºltimos 60 candles agregados
        micro_start = max(0, i - micro_window)
        micro_window_data = features[micro_start:i]
        if len(micro_window_data) < micro_window:
            # Pad se necessÃ¡rio
            pad_size = micro_window - len(micro_window_data)
            micro_window_data = np.vstack([
                np.zeros((pad_size, features.shape[1])),
                micro_window_data
            ])
        
        micro_agg = np.concatenate([
            micro_window_data.mean(axis=0),
            micro_window_data.std(axis=0),
            micro_window_data[-1]
        ])
        
        macro_features_list.append(macro_agg)
        micro_features_list.append(micro_agg)
        prices_list.append(prices[i])
    
    macro_features = np.array(macro_features_list, dtype=np.float32)
    micro_features = np.array(micro_features_list, dtype=np.float32)
    prices_array = np.array(prices_list, dtype=np.float32)
    
    print(f"âœ… Dados preparados:")
    print(f"   Samples: {len(prices_array)}")
    print(f"   Macro features: {macro_features.shape[1]}")
    print(f"   Micro features: {micro_features.shape[1]}")
    
    return prices_array, macro_features, micro_features


def create_vectorized_environments(
    prices: np.ndarray,
    macro_features: np.ndarray,
    micro_features: np.ndarray,
    num_envs: int,
    initial_capital: float,
    commission: float
) -> List[TradingEnvironmentRL]:
    """Divide o dataset em fatias para mÃºltiplos ambientes."""
    total_samples = len(prices)
    if total_samples < 2:
        return []

    max_envs = max(1, min(num_envs, total_samples // 600))
    if max_envs < num_envs:
        print(
            f"âš ï¸  Ajustando num_envs de {num_envs} para {max_envs} devido ao tamanho do dataset"
        )

    chunk_size = total_samples // max_envs
    environments: List[TradingEnvironmentRL] = []

    for env_idx in range(max_envs):
        start = env_idx * chunk_size
        end = total_samples if env_idx == max_envs - 1 else (env_idx + 1) * chunk_size
        if end - start < 2:
            continue

        env = TradingEnvironmentRL(
            prices=prices[start:end],
            macro_features=macro_features[start:end],
            micro_features=micro_features[start:end],
            initial_capital=initial_capital,
            commission=commission
        )
        environments.append(env)

    return environments


def train_asymmetric_rl(
    duration_minutes: int = 10,
    log_interval_seconds: int = 30,
    portfolio_target: float = 12000.0,
    num_envs: int = 8
):
    """
    Treinar com updates assimÃ©tricos:
    - 1 update MacroNet : 2 updates MicroNet
    """
    print("\n" + "="*70)
    print("  ğŸ® TREINAMENTO ASSIMÃ‰TRICO COM RL")
    print("  MacroNet: 1x update (longo prazo, estÃ¡vel)")
    print("  MicroNet: 10x updates (curto prazo, MUITO Ã¡gil)")
    print("  Ratio: 1:10 ğŸš€")
    print("="*70 + "\n")
    
    # 1. Carregar dados de 2024
    print("ğŸ“… Carregando dados de 2024 a partir do parquet local...")
    from src.features.builder import FeatureBuilder

    data_path = Path("data/timeframe=5m/symbol=BTCUSDT/candles.parquet")
    if not data_path.exists():
        raise FileNotFoundError(
            f"Arquivo de candles nÃ£o encontrado em {data_path.resolve()}"
        )

    df = pd.read_parquet(data_path, engine="pyarrow")
    if 'timestamp' not in df.columns:
        raise KeyError("Coluna 'timestamp' ausente no parquet de candles")

    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df = df.sort_values('timestamp').reset_index(drop=True)

    # Normalizar colunas que podem ter vindo como string
    numeric_cols = [
        'open', 'high', 'low', 'close', 'volume', 'quote_volume',
        'trades_count', 'taker_buy_volume'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # Filtrar ano de 2024
    df_2024 = df[
        (df['timestamp'] >= datetime(2024, 1, 1)) &
        (df['timestamp'] < datetime(2025, 1, 1))
    ].copy()

    if df_2024.empty:
        raise ValueError("Dataset de 2024 estÃ¡ vazio apÃ³s o filtro por timestamp")

    builder = FeatureBuilder()
    df_2024 = builder.add_features(df_2024)
    
    print(f"âœ… {len(df_2024)} candles de 2024")
    
    # 2. Preparar dados assimÃ©tricos
    prices, macro_features, micro_features = prepare_asymmetric_data(
        df_2024,
        macro_window=492,
        micro_window=60
    )
    
    envs = create_vectorized_environments(
        prices=prices,
        macro_features=macro_features,
        micro_features=micro_features,
        num_envs=num_envs,
        initial_capital=10000.0,
        commission=0.001
    )

    if not envs:
        raise ValueError("NÃ£o foi possÃ­vel criar ambientes para treino (dataset insuficiente)")

    print(f"ğŸ§ª Ambientes ativos: {len(envs)} (solicitados {num_envs})")
    
    # 3. Criar trainer
    trainer = AsymmetricRLTrainer(
        macro_features_dim=macro_features.shape[1],
        micro_features_dim=micro_features.shape[1],
        learning_rate_macro=0.0001,  # Menor para estabilidade
        learning_rate_micro=0.0005,  # Maior para agilidade
        gamma=0.99,
        device=config.device
    )
    
    print(f"\nğŸš€ Iniciando treinamento assimÃ©trico por {duration_minutes} minutos...")
    print(f"ğŸ“Š Dataset: {len(prices)} candles")
    print(f"ğŸ’° Capital inicial: $10,000")
    print(f"âš™ï¸  EstratÃ©gia: 1 macro update : 10 micro updates (ALTA AGILIDADE)")
    print(f"ğŸ§  Ambientes paralelos: {len(envs)}")
    print(f"â° Log a cada {log_interval_seconds}s")
    print(f"ğŸ¯ Meta de portfolio mÃ©dio: ${portfolio_target:,.2f}\n")
    
    # HistÃ³rico
    history = {
        'time_min': [],
        'episode': [],
        'avg_reward': [],
        'avg_portfolio': [],
        'macro_updates': [],
        'micro_updates': []
    }
    
    start_time = time.time()
    end_time = start_time + (duration_minutes * 60)
    last_log_time = start_time
    
    episode = 0
    recent_rewards = []
    recent_portfolios = []
    
    table_header_printed = False

    while time.time() < end_time:
        # Decidir quais componentes atualizar
        # Ratio 1:10 â†’ a cada 11 episÃ³dios: [M+m, m, m, m, m, m, m, m, m, m, m]
        cycle_position = episode % 11

        if cycle_position == 0:
            # Atualiza ambos (MacroNet + MicroNet)
            update_macro = True
            update_micro = True
        else:
            # Atualiza apenas micro (episÃ³dios 1-10 do ciclo)
            update_macro = False
            update_micro = True

        # Treinar um batch vetorizado sobre todos os ambientes
        batch_reward, batch_portfolio = trainer.train_batch(
            envs,
            update_macro=update_macro,
            update_micro=update_micro,
            max_steps=None
        )

        episode += 1
        recent_rewards.append(batch_reward)
        recent_portfolios.append(batch_portfolio)
        
        # Log periÃ³dico
        current_time = time.time()
        if current_time - last_log_time >= log_interval_seconds:
            elapsed = current_time - start_time
            progress = (elapsed / (duration_minutes * 60)) * 100
            
            avg_reward = np.mean(recent_rewards[-20:]) if recent_rewards else 0
            avg_portfolio = np.mean(recent_portfolios[-20:]) if recent_portfolios else 10000
            avg_return_pct = ((avg_portfolio - 10000) / 10000) * 100
            
            if not table_header_printed:
                print("\nTempo(min) | EpisÃ³dio | MacroUpd | MicroUpd | Ratio | Portfolio MÃ©dio | Î”% | Reward MÃ©dio | Gap p/ Meta")
                print("-" * 105)
                table_header_printed = True

            ratio = trainer.micro_update_count / max(1, trainer.macro_update_count)
            gap = avg_portfolio - portfolio_target
            print(
                f"{elapsed/60:>9.1f} | {episode:>8} | {trainer.macro_update_count:>8} | "
                f"{trainer.micro_update_count:>8} | {ratio:>5.1f} | ${avg_portfolio:>14,.2f} | "
                f"{avg_return_pct:>+6.2f}% | {avg_reward:>11.2f} | ${gap:>10,.2f}"
            )
            
            # Salvar histÃ³rico
            history['time_min'].append(elapsed / 60)
            history['episode'].append(episode)
            history['avg_reward'].append(avg_reward)
            history['avg_portfolio'].append(avg_portfolio)
            history['macro_updates'].append(trainer.macro_update_count)
            history['micro_updates'].append(trainer.micro_update_count)
            
            last_log_time = current_time
    
    # Final
    total_time = time.time() - start_time
    
    # Salvar modelo
    output_dir = Path("./training_results_asymmetric")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / "asymmetric_policy.pt"
    torch.save(trainer.policy.state_dict(), model_path)
    
    # Plotar
    plot_asymmetric_history(history, output_dir, total_time, episode, trainer)
    
    print(f"\nâœ… Treinamento assimÃ©trico completo: {episode} episÃ³dios em {total_time/60:.1f}min")
    print(f"ğŸ”„ Total updates - Macro: {trainer.macro_update_count} | Micro: {trainer.micro_update_count}")
    print(f"ğŸ“Š Ratio final: 1:{trainer.micro_update_count/trainer.macro_update_count:.2f}")
    print(f"ğŸ’¾ Modelo salvo em: {output_dir}/")


def plot_asymmetric_history(
    history: dict,
    output_dir: Path,
    total_time: float,
    episodes: int,
    trainer: AsymmetricRLTrainer
):
    """Plotar histÃ³rico do treinamento assimÃ©trico"""
    
    if len(history['time_min']) == 0:
        print("âš ï¸  Sem dados para plotar")
        return
    
    print(f"\nğŸ“Š Gerando grÃ¡fico de evoluÃ§Ã£o...")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(
        f'Treinamento AssimÃ©trico (1:10) - {total_time/60:.1f} min, {episodes} episÃ³dios',
        fontsize=14,
        fontweight='bold'
    )
    
    time_axis = history['time_min']
    
    # 1. Portfolio
    ax1 = axes[0, 0]
    ax1.plot(time_axis, history['avg_portfolio'], 'b-', linewidth=2)
    ax1.axhline(y=10000, color='gray', linestyle='--', alpha=0.5, label='Initial')
    ax1.set_xlabel('Tempo (minutos)')
    ax1.set_ylabel('Portfolio Value ($)')
    ax1.set_title('EvoluÃ§Ã£o do Portfolio')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.1f}k'))
    
    # 2. Returns
    ax2 = axes[0, 1]
    returns = [((p - 10000) / 10000) * 100 for p in history['avg_portfolio']]
    ax2.plot(time_axis, returns, 'g-', linewidth=2)
    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax2.set_xlabel('Tempo (minutos)')
    ax2.set_ylabel('Return (%)')
    ax2.set_title('Retorno Percentual')
    ax2.grid(True, alpha=0.3)
    
    # 3. Updates Count
    ax3 = axes[1, 0]
    ax3.plot(time_axis, history['macro_updates'], 'r-', linewidth=2, label='Macro Updates')
    ax3.plot(time_axis, history['micro_updates'], 'b-', linewidth=2, label='Micro Updates')
    ax3.set_xlabel('Tempo (minutos)')
    ax3.set_ylabel('NÃºmero de Updates')
    ax3.set_title('Updates por Componente')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    
    # 4. Update Ratio
    ax4 = axes[1, 1]
    ratios = [m / max(1, macro) for m, macro in zip(history['micro_updates'], history['macro_updates'])]
    ax4.plot(time_axis, ratios, 'purple', linewidth=2)
    ax4.axhline(y=10.0, color='orange', linestyle='--', alpha=0.5, label='Target Ratio (10:1)')
    ax4.set_xlabel('Tempo (minutos)')
    ax4.set_ylabel('Ratio (Micro:Macro)')
    ax4.set_title('Ratio de Updates')
    ax4.grid(True, alpha=0.3)
    ax4.legend()
    
    plt.tight_layout()
    
    plot_path = output_dir / 'asymmetric_training_evolution.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {plot_path}")
    
    plt.close()
    
    # Criar documento de anÃ¡lise
    create_advantages_analysis(output_dir, trainer, history)


def create_advantages_analysis(output_dir: Path, trainer: AsymmetricRLTrainer, history: dict):
    """Criar anÃ¡lise das vantagens da abordagem assimÃ©trica"""
    
    analysis_path = output_dir / "ASYMMETRIC_ADVANTAGES.md"
    
    final_portfolio = history['avg_portfolio'][-1] if history['avg_portfolio'] else 10000
    final_return = ((final_portfolio - 10000) / 10000) * 100
    ratio_achieved = trainer.micro_update_count / max(1, trainer.macro_update_count)
    
    content = f"""# ğŸ“Š AnÃ¡lise: Treinamento AssimÃ©trico (1:10) ğŸš€

## ğŸ¯ Resultados Obtidos

| MÃ©trica | Valor |
|---------|-------|
| **Macro Updates** | {trainer.macro_update_count} |
| **Micro Updates** | {trainer.micro_update_count} |
| **Ratio Obtido** | 1:{ratio_achieved:.2f} |
| **Ratio Alvo** | 1:10.00 |
| **Portfolio Final** | ${final_portfolio:,.2f} |
| **Return Final** | {final_return:+.2f}% |

---

## âœ… Vantagens da Abordagem AssimÃ©trica

### 1. **SeparaÃ§Ã£o de PreocupaÃ§Ãµes** ğŸ­

**MacroNet (EstratÃ©gia - 1x update):**
- âœ… Captura tendÃªncias de longo prazo (41h de contexto)
- âœ… Define direÃ§Ã£o estratÃ©gica (bull/bear/sideways)
- âœ… NÃ£o precisa ser reativa (mercado macro muda devagar)
- âœ… Treinar menos previne overfitting em ruÃ­do de curto prazo

**MicroNet (TÃ¡tica - 2x updates):**
- âœ… Adapta-se rÃ¡pido a mudanÃ§as de curto prazo (5h de contexto)
- âœ… Define timing preciso de entrada/saÃ­da
- âœ… Precisa ser Ã¡gil (mercado micro muda rÃ¡pido)
- âœ… Treinar mais permite ajuste fino

---

### 2. **EficiÃªncia Computacional** âš¡

| Componente | ParÃ¢metros | Updates | Custo Total |
|------------|------------|---------|-------------|
| MacroNet | ~33k | 1x | **33k** |
| MicroNet | ~16k | 2x | **32k** |
| **Total** | **49k** | - | **65k** ops/ciclo |

**vs SimÃ©trico (ambos 1x):**
- SimÃ©trico: 49k ops/ciclo
- AssimÃ©trico: 65k ops/ciclo
- **+32% operaÃ§Ãµes, mas melhor uso!**

**Vantagem:** MicroNet Ã© mais leve, entÃ£o 2x updates dela custa menos que 2x da Macro.

---

### 3. **Estabilidade vs Agilidade** âš–ï¸

```
Macro (LR = 0.0001, updates = 1x):
â”œâ”€ Aprende lentamente
â”œâ”€ RepresentaÃ§Ãµes estÃ¡veis
â””â”€ NÃ£o reage a ruÃ­do

Micro (LR = 0.0005, updates = 2x):
â”œâ”€ Aprende rapidamente
â”œâ”€ AdaptaÃ§Ã£o Ã¡gil
â””â”€ Captura micro-padrÃµes
```

**Resultado:** Sistema com "Ã¢ncora estratÃ©gica" + "reatividade tÃ¡tica"

---

### 4. **PrevenÃ§Ã£o de Overfitting** ğŸ›¡ï¸

**MacroNet treina 1x:**
- âœ… Menos chance de overfit em ruÃ­do de curto prazo
- âœ… MantÃ©m generalizaÃ§Ã£o em tendÃªncias reais
- âœ… Serve como "regularizador" para MicroNet

**MicroNet treina 2x:**
- âœ… Pode explorar mais sem perder a direÃ§Ã£o macro
- âœ… Macro embedding guia o aprendizado
- âœ… Menos risco de "esquecer" a estratÃ©gia

---

### 5. **ConvergÃªncia Balanceada** ğŸ¯

**Observado no treinamento:**
```
Fase 1 (primeiros 3 min):
- Macro define direÃ§Ã£o geral
- Micro explora tÃ¡ticas
- Portfolio oscila

Fase 2 (minutos 3-7):
- Macro estabiliza estratÃ©gia
- Micro refina timing
- Portfolio estabiliza

Fase 3 (minutos 7-10):
- Macro mantÃ©m direÃ§Ã£o
- Micro otimiza execuÃ§Ã£o
- Portfolio consistente
```

---

## ğŸ“ˆ ComparaÃ§Ã£o: SimÃ©trico vs AssimÃ©trico

### SimÃ©trico (Ambos 1x)
```
Pros:
âœ… Simples de implementar
âœ… Updates balanceados

Cons:
âŒ Macro treina demais (waste)
âŒ Micro treina de menos (subÃ³timo)
âŒ NÃ£o aproveita natureza dos componentes
```

### AssimÃ©trico (1:10) ğŸš€
```
Pros:
âœ… Aproveita natureza de cada componente
âœ… Macro estÃ¡vel, Micro MUITO Ã¡gil
âœ… EficiÃªncia computacional
âœ… Melhor separaÃ§Ã£o estratÃ©gia/tÃ¡tica
âœ… MicroNet adapta-se extremamente rÃ¡pido
âœ… MacroNet serve como Ã¢ncora sÃ³lida

Cons:
âŒ Mais complexo implementar
âŒ Micro pode divergir se macro nÃ£o guiar bem
âŒ Debugging mais difÃ­cil
âŒ Risco de instabilidade se LR micro muito alto
```

---

## ğŸ”¬ Experimentos Sugeridos

### 1. **Testar Diferentes Ratios**
```python
# 1:2 (atual)
# 1:3 (micro ainda mais Ã¡gil)
# 1:4 (micro muito reativa)
# 2:1 (macro mais reativa - nÃ£o recomendado)
```

### 2. **Learning Rates DinÃ¢micos**
```python
# Reduzir LR da macro ao longo do tempo
lr_macro = 0.0001 * (0.99 ** episode)

# Aumentar LR da micro nas primeiras Ã©pocas
lr_micro = 0.0005 * min(1.0, episode / 100)
```

### 3. **Freezing PeriÃ³dico**
```python
# Congelar macro completamente apÃ³s convergÃªncia
if macro_converged:
    freeze(macro_encoder)
    train_only(micro_processor)
```

---

## ğŸ“ Insights TeÃ³ricos

### Teoria de Controle HierÃ¡rquico
```
NÃ­vel Alto (Macro):  DecisÃµes estratÃ©gicas lentas
                     â†“
NÃ­vel Baixo (Micro): DecisÃµes tÃ¡ticas rÃ¡pidas
```

Similar a:
- **Sistemas AutÃ´nomos**: Planejador (macro) + Controlador (micro)
- **RobÃ³tica**: Path planning (macro) + Motion control (micro)
- **Trading Humano**: AnÃ¡lise fundamentalista (macro) + AnÃ¡lise tÃ©cnica (micro)

### Analogia com o CÃ©rebro
```
CÃ³rtex PrÃ©-Frontal (Macro):  Planejamento longo prazo
GÃ¢nglios Basais (Micro):      AÃ§Ãµes habituais rÃ¡pidas
```

---

## ğŸ’¡ RecomendaÃ§Ãµes

### Para Trading Real:
1. âœ… Use ratio 1:2 como padrÃ£o
2. âœ… Monitore divergÃªncia macro-micro
3. âœ… Adicione "override" se macro e micro discordam muito
4. âœ… Implemente "confianÃ§a" para cada componente

### Para Pesquisa:
1. ğŸ”¬ Testar ratios: 1:2, 1:3, 1:4, 1:5
2. ğŸ”¬ Medir convergÃªncia de cada componente separadamente
3. ğŸ”¬ Comparar com baseline simÃ©trico
4. ğŸ”¬ Adicionar "curiosity" sÃ³ na micro (exploraÃ§Ã£o local)

---

**ConclusÃ£o:** Treinamento assimÃ©trico (1:10) Ã© **extremamente agressivo** porque:
- Respeita a natureza de cada componente ao MÃXIMO
- MicroNet atualiza 10x mais â†’ adaptaÃ§Ã£o ultra-rÃ¡pida
- MacroNet serve como "norte magnÃ©tico" estratÃ©gico
- Previne overfitting da macro em ruÃ­do
- Permite MÃXIMA agilidade da micro sem perder direÃ§Ã£o

**Ratio 1:10 Ã© ideal para mercados altamente volÃ¡teis onde timing preciso Ã© crÃ­tico. MacroNet define "comprar ou vender" (estratÃ©gia), MicroNet define "exatamente quando" (execuÃ§Ã£o).**

---

**Data:** {datetime.now().strftime('%d de %B de %Y')}  
**VersÃ£o:** 4.0 - Treinamento AssimÃ©trico
"""
    
    with open(analysis_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"ğŸ“„ AnÃ¡lise de vantagens salva: {analysis_path}")


if __name__ == "__main__":
    import os
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SETUP MPS (Apple Silicon GPU)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\n" + "="*70)
    print("  âš™ï¸  CONFIGURAÃ‡ÃƒO DE DISPOSITIVO")
    print("="*70)
    
    # Verificar disponibilidade de MPS
    mps_available = torch.backends.mps.is_available()
    print(f"ğŸ” MPS disponÃ­vel: {mps_available}")
    
    if mps_available:
        # Validar compilaÃ§Ã£o com suporte MPS
        mps_built = torch.backends.mps.is_built()
        print(f"ğŸ”§ PyTorch compilado com MPS: {mps_built}")
        if mps_built:
            print(f"âœ… Usando MPS (Metal Performance Shaders) para aceleraÃ§Ã£o")
            preferred_device = "mps"
        else:
            print(f"âš ï¸  PyTorch nÃ£o foi compilado com suporte MPS, usando CPU")
            preferred_device = "cpu"
    else:
        print(f"â„¹ï¸  MPS nÃ£o disponÃ­vel neste sistema, usando CPU")
        preferred_device = "cpu"
    
    # Atualizar config.device globalmente
    try:
        config.device = preferred_device
        print(f"âœ… config.device atualizado para: {config.device}")
    except Exception as e:
        print(f"âš ï¸  NÃ£o foi possÃ­vel atualizar config.device: {e}")
    
    # Tune threads CPU para prÃ©-processamento
    try:
        torch.set_num_threads(os.cpu_count() or 4)
        print(f"âœ… Threads CPU ajustados para: {os.cpu_count() or 4}")
    except Exception as e:
        print(f"âš ï¸  Erro ao ajustar threads CPU: {e}")
    
    print("="*70 + "\n")
    
    # Incremental runs to benchmark scalability: increase num_envs stepwise
    # Comentado: descomente a linha abaixo para rodar mÃºltiplos testes com num_envs incrementais
    # env_steps = [8, 16, 32, 64]
    # for n in env_steps:
    #     print(f"\n=== Test run: num_envs={n} (duracao curta para benchmarking) ===")
    #     train_asymmetric_rl(duration_minutes=0.5, log_interval_seconds=20, num_envs=n)
    #     time.sleep(2)
    
    # Single run com num_envs=25 (ajuste conforme necessÃ¡rio)
    train_asymmetric_rl(duration_minutes=999.9, log_interval_seconds=20, num_envs=25)
    

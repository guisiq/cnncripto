"""
Pipeline de Treinamento Completo com M√©tricas Reais e Early Stopping

Este pipeline implementa:
- Early stopping baseado em m√©tricas de valida√ß√£o
- Checkpointing de melhores modelos
- Monitoramento de overfitting
- M√©tricas de avalia√ß√£o em tempo real
- Logging detalhado
- Valida√ß√£o cruzada temporal
"""

import numpy as np
import pandas as pd
from pathlib import Path
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
import torch

from src.pipeline import TradingPipeline
from src.config import config
from src.evaluation.backtest import SimpleBacktester, calculate_metrics
from src.logger import get_logger

logger = get_logger(__name__)


@dataclass
class TrainingMetrics:
    """M√©tricas de treinamento por √©poca"""
    epoch: int
    train_loss: float
    val_loss: float
    sharpe_ratio: float
    total_return: float
    max_drawdown: float
    win_rate: float
    num_trades: int
    timestamp: str


@dataclass
class EarlyStoppingConfig:
    """Configura√ß√£o de early stopping"""
    patience: int = 10  # √âpocas sem melhoria antes de parar
    min_delta: float = 0.001  # Melhoria m√≠nima considerada
    monitor_metric: str = 'val_loss'  # M√©trica a monitorar
    mode: str = 'min'  # 'min' ou 'max'
    restore_best_weights: bool = True


class EarlyStopping:
    """Early stopping para prevenir overfitting"""
    
    def __init__(self, config: EarlyStoppingConfig):
        self.config = config
        self.best_value = float('inf') if config.mode == 'min' else float('-inf')
        self.best_epoch = 0
        self.counter = 0
        self.should_stop = False
        self.best_weights_path = None
    
    def __call__(self, current_value: float, epoch: int, model_path: str) -> bool:
        """
        Verifica se deve parar o treinamento
        
        Returns:
            True se deve parar, False caso contr√°rio
        """
        improved = False
        
        if self.config.mode == 'min':
            if current_value < self.best_value - self.config.min_delta:
                improved = True
        else:
            if current_value > self.best_value + self.config.min_delta:
                improved = True
        
        if improved:
            self.best_value = current_value
            self.best_epoch = epoch
            self.counter = 0
            self.best_weights_path = model_path
            logger.info(
                "early_stopping_improvement",
                metric=self.config.monitor_metric,
                value=current_value,
                epoch=epoch
            )
        else:
            self.counter += 1
            logger.info(
                "early_stopping_no_improvement",
                counter=self.counter,
                patience=self.config.patience
            )
        
        if self.counter >= self.config.patience:
            self.should_stop = True
            logger.info(
                "early_stopping_triggered",
                best_epoch=self.best_epoch,
                best_value=self.best_value
            )
        
        return self.should_stop


class TrainingPipelineWithMetrics:
    """Pipeline de treinamento completo com m√©tricas e valida√ß√£o"""
    
    def __init__(
        self,
        symbol: str = "BTCUSDT",
        output_dir: str = "./training_results",
        val_split: float = 0.2,
        early_stopping_config: Optional[EarlyStoppingConfig] = None
    ):
        self.symbol = symbol
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.val_split = val_split
        
        self.pipeline = TradingPipeline()
        self.backtester = SimpleBacktester(
            initial_cash=10000,
            commission=config.backtest.commission
        )
        
        self.early_stopping_config = early_stopping_config or EarlyStoppingConfig()
        self.training_history: List[TrainingMetrics] = []
        
        logger.info(
            "training_pipeline_initialized",
            symbol=symbol,
            output_dir=str(self.output_dir),
            val_split=val_split
        )
    
    def prepare_data(
        self,
        days_back: int = 30
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Prepara dados de treino e valida√ß√£o com split temporal
        
        Returns:
            train_data, val_data, full_data
        """
        logger.info("preparing_data", days_back=days_back)
        
        long_data, short_data, full_df = self.pipeline.fetch_and_prepare_data(
            self.symbol,
            days_back=days_back
        )
        
        # Split temporal: √∫ltimos X% para valida√ß√£o
        n_samples = len(full_df)
        split_idx = int(n_samples * (1 - self.val_split))
        
        train_data = full_df.iloc[:split_idx].copy()
        val_data = full_df.iloc[split_idx:].copy()
        
        logger.info(
            "data_prepared",
            total_samples=n_samples,
            train_samples=len(train_data),
            val_samples=len(val_data)
        )
        
        return train_data, val_data, full_df
    
    def prepare_training_data(
        self,
        train_data: pd.DataFrame,
        future_horizon: int = 5
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prepara dados para treinamento end-to-end
        Usa retornos futuros de m√∫ltiplos candles (n√£o apenas pr√≥ximo)
        
        Args:
            future_horizon: Quantos candles √† frente considerar
        
        Returns:
            X_short_train, y_train (X_macro ser√° gerado a cada √©poca)
        """
        lookback = config.micronet.lookback_candles
        X_short_list = []
        y_list = []
        
        for i in range(lookback, len(train_data) - future_horizon):
            window = train_data.iloc[i-lookback:i]
            X_short = self.pipeline.extract_feature_arrays(window)
            
            if X_short.shape[0] < lookback:
                continue
            
            X_short_list.append(X_short)
            
            # Target: retorno acumulado dos pr√≥ximos N candles
            # Isso d√° mais sinal para a rede aprender
            current_price = train_data.iloc[i]['close']
            future_prices = train_data.iloc[i+1:i+1+future_horizon]['close'].values
            
            # M√°ximo retorno no horizonte (para long) ou m√≠nimo (para short)
            max_return = (future_prices.max() - current_price) / current_price
            min_return = (future_prices.min() - current_price) / current_price
            
            # Target: melhor dire√ß√£o (long se max_return > abs(min_return))
            if max_return > abs(min_return) and max_return > 0.001:  # >0.1%
                target = np.tanh(max_return * 100)  # Long signal
            elif abs(min_return) > max_return and min_return < -0.001:  # <-0.1%
                target = -np.tanh(abs(min_return) * 100)  # Short signal
            else:
                target = 0.0  # Neutro
            
            y_list.append(target)
        
        X_short_train = np.array(X_short_list, dtype=np.float32)
        y_train = np.array(y_list, dtype=np.float32).reshape(-1, 1)
        
        return X_short_train, y_train
    
    def evaluate_on_backtest(
        self,
        data: pd.DataFrame,
        prefix: str = "eval"
    ) -> Dict:
        """
        Avalia modelo em backtest
        
        Returns:
            Dict com m√©tricas de backtest
        """
        logger.info(f"{prefix}_backtest_evaluation")
        
        # Gerar sinais para todos os candles
        signals = []
        prices = data['close'].values
        
        # Gerar embedding macro uma vez
        macro_embedding = self.pipeline.generate_macro_embedding(
            self.symbol,
            days_back=5
        )
        
        # Simular predi√ß√µes sequenciais
        lookback = config.micronet.lookback_candles
        
        for i in range(lookback, len(data)):
            window = data.iloc[i-lookback:i]
            X_short = self.pipeline.extract_feature_arrays(window)
            
            # Garantir shape correto
            if X_short.shape[0] < lookback:
                pad_size = lookback - X_short.shape[0]
                X_short = np.vstack([
                    np.zeros((pad_size, X_short.shape[1])),
                    X_short
                ]).astype(np.float32)
            
            X_short = X_short[np.newaxis, :, :].astype(np.float32)
            macro_emb = macro_embedding[np.newaxis, :].astype(np.float32)
            
            try:
                signal = self.pipeline.micronet.predict(X_short, macro_emb)[0]
                signals.append(float(signal))
            except:
                signals.append(0.0)
        
        # Pad in√≠cio com zeros
        signals = [0.0] * lookback + signals
        signals = np.array(signals)
        
        # Debug: estat√≠sticas dos sinais
        signal_stats = {
            'mean': float(np.mean(signals)),
            'std': float(np.std(signals)),
            'min': float(np.min(signals)),
            'max': float(np.max(signals)),
            'above_02': int(np.sum(np.abs(signals) > 0.2)),
            'above_05': int(np.sum(np.abs(signals) > 0.5))
        }
        
        # Executar backtest com threshold mais baixo para capturar mais trades
        # Tanh gera valores em [-1, 1], ent√£o 0.5 √© muito alto
        results = self.backtester.backtest(
            prices,
            signals,
            signal_threshold=0.2  # Threshold mais baixo para gerar mais sinais
        )
        
        logger.info(
            f"{prefix}_backtest_results",
            total_return=results['total_return'],
            sharpe=results['sharpe_ratio'],
            max_dd=results['max_drawdown'],
            trades=results['num_trades'],
            **signal_stats
        )
        
        return results
    
    def train_end_to_end_with_validation(
        self,
        train_data: pd.DataFrame,
        val_data: pd.DataFrame,
        max_epochs: int = 200,
        batch_size: int = 32
    ) -> Dict:
        """
        Treina MacroNet + MicroNet end-to-end com mesmo fitness (backtest)
        
        A cada √©poca:
        1. Atualiza MacroNet (encoder)
        2. Gera novos embeddings macro
        3. Treina MicroNet com novos embeddings
        4. Avalia sistema completo em backtest
        
        Returns:
            Dict com hist√≥rico de treinamento
        """
        logger.info("training_end_to_end", max_epochs=max_epochs)
        
        early_stopping = EarlyStopping(
            EarlyStoppingConfig(
                patience=20,
                monitor_metric='sharpe_ratio',
                mode='max'
            )
        )
        
        history = []
        
        # Preparar dados de treino longos para MacroNet
        X_long_train = self.pipeline.extract_feature_arrays(train_data)
        X_long_train = X_long_train[np.newaxis, :, :].astype(np.float32)
        
        # Construir MacroNet primeiro
        input_dim = X_long_train.shape[2]
        if self.pipeline.macronet.model is None:
            self.pipeline.macronet.build_model(input_dim)
            logger.info("macronet_built", input_dim=input_dim)
        
        # Preparar dados curtos para MicroNet
        X_short_train, y_train = self.prepare_training_data(train_data)
        
        # Construir MicroNet
        short_dim = X_short_train.shape[1] * X_short_train.shape[2]
        if self.pipeline.micronet.model is None:
            self.pipeline.micronet.build_model(
                short_features_dim=short_dim,
                macro_embedding_dim=128
            )
            logger.info("micronet_built", short_dim=short_dim)
        
        logger.info(
            "data_prepared",
            long_samples=X_long_train.shape[1],
            short_samples=len(X_short_train),
            features=X_short_train.shape[2]
        )
        
        # Track melhor loss para ajustar learning rate
        prev_val_sharpe = float('-inf')
        stagnation_counter = 0
        
        # Fase de explora√ß√£o inicial: adicionar ru√≠do aos targets
        exploration_epochs = 20
        
        for epoch in range(max_epochs):
            # 1. Treinar MacroNet por 1 √©poca (aprender melhores representa√ß√µes)
            self.pipeline.macronet.train(X_long_train, epochs=1, batch_size=batch_size)
            
            # 2. Gerar novos embeddings macro com encoder atualizado
            macro_embedding = self.pipeline.macronet.encode(X_long_train)[0]
            X_macro_train = np.tile(
                macro_embedding[np.newaxis, :],
                (len(X_short_train), 1)
            ).astype(np.float32)
            
            # 3. Preparar targets com explora√ß√£o inicial
            y_train_epoch = y_train.copy()
            if epoch < exploration_epochs:
                # Adicionar ru√≠do para explora√ß√£o
                noise_scale = 0.3 * (1 - epoch / exploration_epochs)
                noise = np.random.normal(0, noise_scale, y_train.shape).astype(np.float32)
                y_train_epoch = np.clip(y_train + noise, -1, 1)
            
            # 3. Treinar MicroNet com embeddings atualizados (mais √©pocas para dar tempo de aprender)
            self.pipeline.micronet.train(
                X_short_train,
                X_macro_train,
                y_train_epoch,
                epochs=3,  # Mais √©pocas para MicroNet aprender melhor
                batch_size=batch_size
            )
            
            # 4. Avaliar sistema completo em backtest a cada 2 √©pocas (mais frequente)
            if (epoch + 1) % 2 == 0:
                val_results = self.evaluate_on_backtest(val_data, prefix="val")
                train_results = self.evaluate_on_backtest(train_data, prefix="train")
                
                # Calcular reconstruction loss do MacroNet
                X_long_prepared = self.pipeline.macronet._prepare_input_array(X_long_train)
                X_long_tensor = torch.from_numpy(X_long_prepared).to(
                    self.pipeline.macronet.device_obj
                )
                with torch.no_grad():
                    recon, _ = self.pipeline.macronet.model(X_long_tensor)
                    macro_loss = float(
                        ((recon - X_long_tensor) ** 2).mean().cpu()
                    )
                
                metrics = TrainingMetrics(
                    epoch=epoch + 1,
                    train_loss=macro_loss,
                    val_loss=0.0,  # Usamos Sharpe como m√©trica principal
                    sharpe_ratio=val_results['sharpe_ratio'],
                    total_return=val_results['total_return'],
                    max_drawdown=val_results['max_drawdown'],
                    win_rate=val_results.get('win_rate', 0.0),
                    num_trades=val_results['num_trades'],
                    timestamp=datetime.now().isoformat()
                )
                
                history.append(asdict(metrics))
                self.training_history.append(metrics)
                
                logger.info(
                    "epoch_metrics",
                    **asdict(metrics),
                    train_sharpe=train_results['sharpe_ratio']
                )
                
                # Detectar estagna√ß√£o e ajustar learning rate
                if val_results['sharpe_ratio'] <= prev_val_sharpe + 0.01:
                    stagnation_counter += 1
                    if stagnation_counter >= 5:  # 5 avalia√ß√µes sem melhoria
                        # Reduzir learning rate
                        for param_group in self.pipeline.micronet.optimizer.param_groups:
                            old_lr = param_group['lr']
                            param_group['lr'] *= 0.5
                            logger.info("lr_reduced", old_lr=old_lr, new_lr=param_group['lr'])
                        stagnation_counter = 0
                else:
                    stagnation_counter = 0
                    prev_val_sharpe = val_results['sharpe_ratio']
                
                print(f"\n{'='*60}")
                print(f"√âpoca {epoch + 1}/{max_epochs}")
                print(f"{'='*60}")
                print(f"MacroNet Loss:     {macro_loss:.6f}")
                print(f"Val Sharpe:        {val_results['sharpe_ratio']:.2f} {'üî∫' if val_results['sharpe_ratio'] > prev_val_sharpe else 'üîª'}")
                print(f"Val Return:        {val_results['total_return']*100:+.2f}%")
                print(f"Val Max DD:        {val_results['max_drawdown']*100:.2f}%")
                print(f"Val Trades:        {val_results['num_trades']}")
                print(f"Train Sharpe:      {train_results['sharpe_ratio']:.2f}")
                print(f"Stagnation:        {stagnation_counter}/5")
                
                # Debug: mostrar estat√≠sticas dos sinais  
                if (epoch + 1) <= 10 or (epoch + 1) % 10 == 0:
                    print(f"\nüìä Sinais Gerados:")
                    # Pegar stats do logger se dispon√≠vel
                    print(f"   Sinais > |0.2|: ?")
                    print(f"   Sinais > |0.5|: ?")
                
                # Salvar checkpoints
                macro_checkpoint = str(
                    self.output_dir / f"macronet_epoch_{epoch+1}.pt"
                )
                micro_checkpoint = str(
                    self.output_dir / f"micronet_epoch_{epoch+1}.pt"
                )
                self.pipeline.macronet.save_model(macro_checkpoint)
                self.pipeline.micronet.save_model(micro_checkpoint)
                
                # Early stopping baseado em Sharpe ratio de valida√ß√£o
                if early_stopping(
                    val_results['sharpe_ratio'],
                    epoch + 1,
                    micro_checkpoint  # Salvamos ambos, mas referenciamos um
                ):
                    logger.info(
                        "early_stopping_triggered",
                        stopped_at_epoch=epoch + 1,
                        best_epoch=early_stopping.best_epoch
                    )
                    
                    # Restaurar melhores pesos de ambas as redes
                    if early_stopping.config.restore_best_weights:
                        best_macro = str(
                            self.output_dir / f"macronet_epoch_{early_stopping.best_epoch}.pt"
                        )
                        best_micro = str(
                            self.output_dir / f"micronet_epoch_{early_stopping.best_epoch}.pt"
                        )
                        
                        self.pipeline.macronet.load_model(
                            best_macro,
                            input_dim=X_long_train.shape[2]
                        )
                        self.pipeline.micronet.load_model(
                            best_micro,
                            short_features_dim=X_short_train.shape[1] * X_short_train.shape[2],
                            macro_embedding_dim=128
                        )
                        logger.info("best_weights_restored", epoch=early_stopping.best_epoch)
                    
                    break
        
        # Salvar modelos finais
        macro_final = str(self.output_dir / "macronet_final.pt")
        micro_final = str(self.output_dir / "micronet_final.pt")
        self.pipeline.macronet.save_model(macro_final)
        self.pipeline.micronet.save_model(micro_final)
        
        return {
            'history': history,
            'best_epoch': early_stopping.best_epoch,
            'best_sharpe': early_stopping.best_value,
            'macronet_path': macro_final,
            'micronet_path': micro_final
        }
    
    def plot_training_history(self):
        """Plota hist√≥rico de treinamento"""
        if not self.training_history:
            logger.warning("no_training_history_to_plot")
            return
        
        epochs = [m.epoch for m in self.training_history]
        sharpe = [m.sharpe_ratio for m in self.training_history]
        returns = [m.total_return * 100 for m in self.training_history]
        drawdowns = [m.max_drawdown * 100 for m in self.training_history]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Sharpe Ratio
        axes[0, 0].plot(epochs, sharpe, marker='o')
        axes[0, 0].set_title('Sharpe Ratio por √âpoca')
        axes[0, 0].set_xlabel('√âpoca')
        axes[0, 0].set_ylabel('Sharpe Ratio')
        axes[0, 0].grid(True)
        
        # Total Return
        axes[0, 1].plot(epochs, returns, marker='o', color='green')
        axes[0, 1].set_title('Retorno Total por √âpoca (%)')
        axes[0, 1].set_xlabel('√âpoca')
        axes[0, 1].set_ylabel('Retorno (%)')
        axes[0, 1].grid(True)
        
        # Max Drawdown
        axes[1, 0].plot(epochs, drawdowns, marker='o', color='red')
        axes[1, 0].set_title('Max Drawdown por √âpoca (%)')
        axes[1, 0].set_xlabel('√âpoca')
        axes[1, 0].set_ylabel('Drawdown (%)')
        axes[1, 0].grid(True)
        
        # Trades
        trades = [m.num_trades for m in self.training_history]
        axes[1, 1].plot(epochs, trades, marker='o', color='orange')
        axes[1, 1].set_title('N√∫mero de Trades por √âpoca')
        axes[1, 1].set_xlabel('√âpoca')
        axes[1, 1].set_ylabel('Trades')
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        plot_path = self.output_dir / 'training_history.png'
        plt.savefig(plot_path, dpi=150)
        logger.info("training_plot_saved", path=str(plot_path))
        
        plt.close()
    
    def save_training_report(self):
        """Salva relat√≥rio completo de treinamento"""
        report = {
            'symbol': self.symbol,
            'timestamp': datetime.now().isoformat(),
            'config': {
                'val_split': self.val_split,
                'early_stopping': asdict(self.early_stopping_config)
            },
            'training_history': [asdict(m) for m in self.training_history],
            'best_metrics': None
        }
        
        if self.training_history:
            best_idx = max(
                range(len(self.training_history)),
                key=lambda i: self.training_history[i].sharpe_ratio
            )
            report['best_metrics'] = asdict(self.training_history[best_idx])
        
        report_path = self.output_dir / 'training_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info("training_report_saved", path=str(report_path))
    
    def run_full_training(
        self,
        days_back: int = 30,
        max_epochs: int = 200
    ):
        """
        Executa pipeline completo de treinamento END-TO-END
        
        MacroNet e MicroNet s√£o treinadas juntas com o mesmo fitness (backtest)
        
        Args:
            days_back: Dias de dados hist√≥ricos
            max_epochs: M√°ximo de √©pocas para treinamento conjunto
        """
        logger.info(
            "starting_end_to_end_training",
            days_back=days_back,
            max_epochs=max_epochs
        )
        
        print("="*70)
        print("  PIPELINE DE TREINAMENTO END-TO-END")
        print("  MacroNet + MicroNet com Fitness Compartilhado")
        print("="*70)
        
        # 1. Preparar dados
        print("\n[1/3] Preparando dados...")
        train_data, val_data, full_data = self.prepare_data(days_back=days_back)
        print(f"  ‚úì Train: {len(train_data)} candles")
        print(f"  ‚úì Val:   {len(val_data)} candles")
        
        # 2. Treinar ambas as redes end-to-end
        print("\n[2/3] Treinando MacroNet + MicroNet end-to-end...")
        print("  Fitness: Sharpe Ratio em backtest de valida√ß√£o")
        print("  Early stopping: 20 √©pocas sem melhoria")
        
        training_results = self.train_end_to_end_with_validation(
            train_data,
            val_data,
            max_epochs=max_epochs
        )
        
        # 3. Avalia√ß√£o final
        print("\n[3/3] Avalia√ß√£o final...")
        final_val_results = self.evaluate_on_backtest(val_data, prefix="final_val")
        final_train_results = self.evaluate_on_backtest(train_data, prefix="final_train")
        
        print("\n" + "="*70)
        print("  RESULTADOS FINAIS")
        print("="*70)
        print(f"\nMelhor √âpoca: {training_results['best_epoch']}")
        print(f"Melhor Sharpe: {training_results['best_sharpe']:.2f}")
        
        print(f"\nüìä Valida√ß√£o (Out-of-Sample):")
        print(f"  ‚Ä¢ Retorno Total:  {final_val_results['total_return']*100:+.2f}%")
        print(f"  ‚Ä¢ Sharpe Ratio:   {final_val_results['sharpe_ratio']:.2f}")
        print(f"  ‚Ä¢ Max Drawdown:   {final_val_results['max_drawdown']*100:.2f}%")
        print(f"  ‚Ä¢ Win Rate:       {final_val_results.get('win_rate', 0)*100:.2f}%")
        print(f"  ‚Ä¢ Num Trades:     {final_val_results['num_trades']}")
        
        print(f"\nüìà Treino (In-Sample):")
        print(f"  ‚Ä¢ Retorno Total:  {final_train_results['total_return']*100:+.2f}%")
        print(f"  ‚Ä¢ Sharpe Ratio:   {final_train_results['sharpe_ratio']:.2f}")
        print(f"  ‚Ä¢ Max Drawdown:   {final_train_results['max_drawdown']*100:.2f}%")
        print(f"  ‚Ä¢ Num Trades:     {final_train_results['num_trades']}")
        
        # Detectar overfitting
        sharpe_diff = final_train_results['sharpe_ratio'] - final_val_results['sharpe_ratio']
        if sharpe_diff > 1.0:
            print(f"\n‚ö†Ô∏è  AVISO: Poss√≠vel overfitting detectado!")
            print(f"   Diferen√ßa de Sharpe (train - val): {sharpe_diff:.2f}")
        elif final_val_results['sharpe_ratio'] > 1.0:
            print(f"\n‚úÖ Modelo com boa generaliza√ß√£o!")
        
        # Salvar relat√≥rios
        print("\n[4/4] Salvando relat√≥rios...")
        self.plot_training_history()
        self.save_training_report()
        print(f"  ‚úì Relat√≥rios salvos em: {self.output_dir}")
        print(f"  ‚úì MacroNet final: {training_results['macronet_path']}")
        print(f"  ‚úì MicroNet final: {training_results['micronet_path']}")
        
        print("\n" + "="*70)
        print("  ‚úÖ TREINAMENTO END-TO-END COMPLETO!")
        print("="*70)
        
        return {
            'training': training_results,
            'final_validation': final_val_results,
            'final_train': final_train_results
        }


def main():
    """Exemplo de uso - Treinamento End-to-End"""
    
    # Configurar pipeline
    training_pipeline = TrainingPipelineWithMetrics(
        symbol="BTCUSDT",
        output_dir="./training_results",
        val_split=0.2,
        early_stopping_config=EarlyStoppingConfig(
            patience=20,  # Mais paci√™ncia para treinamento conjunto
            min_delta=0.01,
            monitor_metric='sharpe_ratio',
            mode='max'  # Maximizar Sharpe
        )
    )
    
    # Executar treinamento end-to-end
    # MacroNet e MicroNet treinadas juntas com mesmo fitness
    results = training_pipeline.run_full_training(
        days_back=30,
        max_epochs=200
    )
    
    print(f"\nüìä Resultados salvos em: {training_pipeline.output_dir}")
    print(f"üìà Gr√°ficos dispon√≠veis em: {training_pipeline.output_dir}/training_history.png")
    print(f"üìÑ Relat√≥rio JSON em: {training_pipeline.output_dir}/training_report.json")
    print(f"\nüí° Ambas as redes foram treinadas com o mesmo fitness (Sharpe Ratio)!")
    print(f"   Isso garante que MacroNet aprende representa√ß√µes √∫teis para trading.")


if __name__ == "__main__":
    main()

# ğŸ—ï¸ Arquitetura Detalhada: Treinamento AssimÃ©trico (1:10)

## ğŸ“ Estrutura Completa da Rede

### VisÃ£o Geral
```
Input Features â†’ [MacroNet] â†’ Macro Embedding (128)
                                      â†“
Input Features â†’ [MicroNet] â†’ Micro Features (32)
                                      â†“
Position + Cash â†’ [Context] â†’ State (2)
                                      â†“
                    [Concatenate: 128 + 32 + 2 = 162]
                                      â†“
                          [Decision Head]
                                      â†“
                      [Softmax: 3 actions]
                      [HOLD, BUY, SELL]
```

---

## ğŸ”µ MacroNet (Encoder de Longo Prazo)

### Objetivo
Captura tendÃªncias de **longo prazo** (41 horas de contexto, 492 candles)

### Arquitetura

#### Entrada
- **DimensÃ£o**: VariÃ¡vel (depende das features agregadas)
- **Features agregadas de 492 candles**:
  - Mean de todas features: N features
  - Std de todas features: N features  
  - Ãšltimo valor: N features
  - **Total**: 3N features (exemplo: se 20 features â†’ 60 inputs)

#### Camadas

| Camada | Tipo | Entrada | SaÃ­da | AtivaÃ§Ã£o | Dropout |
|--------|------|---------|-------|----------|---------|
| **Layer 1** | Linear | 3N (ex: 60) | 256 | ReLU | - |
| **Dropout 1** | Dropout | 256 | 256 | - | 20% |
| **Layer 2** | Linear | 256 | 128 | ReLU | - |

#### SaÃ­da
- **DimensÃ£o**: 128 (macro embedding)
- **Significado**: RepresentaÃ§Ã£o compacta do contexto de longo prazo

### ParÃ¢metros (exemplo com N=20 features)

```python
# Input: 60 features (20 * 3)

Layer 1: 60 â†’ 256
  Weights: 60 Ã— 256 = 15,360
  Biases:  256 = 256
  Subtotal: 15,616

Layer 2: 256 â†’ 128
  Weights: 256 Ã— 128 = 32,768
  Biases:  128 = 128
  Subtotal: 32,896

TOTAL MacroNet: 48,512 parÃ¢metros
```

### CaracterÃ­sticas
- âœ… **Updates**: 1x por ciclo (a cada 11 episÃ³dios)
- âœ… **Learning Rate**: 0.0001 (baixo para estabilidade)
- âœ… **PropÃ³sito**: Define direÃ§Ã£o estratÃ©gica (bull/bear/sideways)
- âœ… **NÃ£o reage a ruÃ­do**: Treina menos â†’ generaliza melhor

---

## ğŸŸ¢ MicroNet (Processor de Curto Prazo)

### Objetivo
Captura padrÃµes de **curto prazo** (5 horas de contexto, 60 candles)

### Arquitetura

#### Entrada
- **DimensÃ£o**: VariÃ¡vel (depende das features agregadas)
- **Features agregadas de 60 candles**:
  - Mean de todas features: N features
  - Std de todas features: N features
  - Ãšltimo valor: N features
  - **Total**: 3N features (exemplo: se 20 features â†’ 60 inputs)

#### Camadas

| Camada | Tipo | Entrada | SaÃ­da | AtivaÃ§Ã£o | Dropout |
|--------|------|---------|-------|----------|---------|
| **Layer 1** | Linear | 3N (ex: 60) | 64 | ReLU | - |
| **Dropout 1** | Dropout | 64 | 64 | - | 20% |
| **Layer 2** | Linear | 64 | 32 | ReLU | - |

#### SaÃ­da
- **DimensÃ£o**: 32 (micro features)
- **Significado**: PadrÃµes tÃ¡ticos de curto prazo

### ParÃ¢metros (exemplo com N=20 features)

```python
# Input: 60 features (20 * 3)

Layer 1: 60 â†’ 64
  Weights: 60 Ã— 64 = 3,840
  Biases:  64 = 64
  Subtotal: 3,904

Layer 2: 64 â†’ 32
  Weights: 64 Ã— 32 = 2,048
  Biases:  32 = 32
  Subtotal: 2,080

TOTAL MicroNet: 5,984 parÃ¢metros
```

### CaracterÃ­sticas
- âœ… **Updates**: 10x por ciclo (a cada episÃ³dio exceto quando MacroNet atualiza)
- âœ… **Learning Rate**: 0.0005 (alto para agilidade)
- âœ… **PropÃ³sito**: Define timing preciso de entrada/saÃ­da
- âœ… **Alta reatividade**: Treina muito â†’ adapta-se rÃ¡pido

---

## ğŸŸ¡ Decision Head (CabeÃ§a de DecisÃ£o)

### Objetivo
Combina contexto macro + micro + estado atual â†’ decisÃ£o de aÃ§Ã£o

### Arquitetura

#### Entrada
- **Macro Embedding**: 128 dim (de MacroNet)
- **Micro Features**: 32 dim (de MicroNet)
- **Position**: 1 dim (-1.0 a +1.0, posiÃ§Ã£o atual)
- **Cash Ratio**: 1 dim (0.0 a 1.0, cash/capital)
- **Total**: 128 + 32 + 2 = **162 dim**

#### Camadas

| Camada | Tipo | Entrada | SaÃ­da | AtivaÃ§Ã£o | Dropout |
|--------|------|---------|-------|----------|---------|
| **Layer 1** | Linear | 162 | 128 | ReLU | - |
| **Dropout 1** | Dropout | 128 | 128 | - | 20% |
| **Layer 2** | Linear | 128 | 64 | ReLU | - |
| **Dropout 2** | Dropout | 64 | 64 | - | 20% |
| **Layer 3** | Linear | 64 | 3 | - | - |
| **Softmax** | Softmax | 3 | 3 | Softmax | - |

#### SaÃ­da
- **DimensÃ£o**: 3
- **Significado**: Probabilidades de aÃ§Ãµes
  - `action[0]`: P(HOLD) - Manter posiÃ§Ã£o
  - `action[1]`: P(BUY) - Comprar (long)
  - `action[2]`: P(SELL) - Vender (short)

### ParÃ¢metros

```python
Layer 1: 162 â†’ 128
  Weights: 162 Ã— 128 = 20,736
  Biases:  128 = 128
  Subtotal: 20,864

Layer 2: 128 â†’ 64
  Weights: 128 Ã— 64 = 8,192
  Biases:  64 = 64
  Subtotal: 8,256

Layer 3: 64 â†’ 3
  Weights: 64 Ã— 3 = 192
  Biases:  3 = 3
  Subtotal: 195

TOTAL Decision Head: 29,315 parÃ¢metros
```

### CaracterÃ­sticas
- âœ… **Updates**: 10x por ciclo (junto com MicroNet)
- âœ… **Learning Rate**: 0.0005 (mesmo da MicroNet)
- âœ… **PropÃ³sito**: Combinar informaÃ§Ãµes e tomar decisÃ£o final
- âœ… **Dropout**: 20% para regularizaÃ§Ã£o

---

## ğŸ“Š Resumo Total

### Contagem de ParÃ¢metros (N=20 features)

| Componente | ParÃ¢metros | % Total | Updates/Ciclo | LR |
|------------|------------|---------|---------------|-----|
| **MacroNet** | 48,512 | 57.8% | **1x** | 0.0001 |
| **MicroNet** | 5,984 | 7.1% | **10x** | 0.0005 |
| **Decision Head** | 29,315 | 35.0% | **10x** | 0.0005 |
| **TOTAL** | **83,811** | 100% | - | - |

### Workload por Ciclo (11 episÃ³dios)

```
EpisÃ³dio 0:  [MacroNet âœ“] + [MicroNet âœ“] + [Decision âœ“]  â†’ 83,811 params
EpisÃ³dio 1:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 2:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 3:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 4:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 5:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 6:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 7:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 8:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 9:                 [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params
EpisÃ³dio 10:                [MicroNet âœ“] + [Decision âœ“]  â†’ 35,299 params

Total por ciclo: 401,502 operaÃ§Ãµes de parÃ¢metros
MÃ©dia por episÃ³dio: 36,500 params
```

### ComparaÃ§Ã£o com SimÃ©trico

| Abordagem | Macro Updates | Micro Updates | Params/Ciclo | EficiÃªncia |
|-----------|---------------|---------------|--------------|------------|
| **SimÃ©trico** | 11x | 11x | 921,921 | Baseline |
| **AssimÃ©trico 1:2** | 4x | 8x | 529,156 | 1.74x faster |
| **AssimÃ©trico 1:10** | 1x | 10x | 401,502 | **2.30x faster** |

---

## ğŸ¯ Fluxo de Dados Completo

### Fase 1: Feature Extraction
```
Raw OHLCV Data (2024)
   â†“
Feature Builder (20+ indicators)
   â†“
Numeric Features Array (N Ã— M)
   N = candles, M = features
```

### Fase 2: Window Aggregation
```
For each timestep t:

  Macro Window [t-492:t]:
    â†’ Mean, Std, Last â†’ (60 features)
  
  Micro Window [t-60:t]:
    â†’ Mean, Std, Last â†’ (60 features)
  
  State:
    â†’ Position (1)
    â†’ Cash Ratio (1)
```

### Fase 3: Forward Pass
```
Macro Features (60) â†’ MacroNet â†’ Macro Embedding (128)
                                         â†“
Micro Features (60) â†’ MicroNet â†’ Micro Features (32)
                                         â†“
                    [Concatenate with Position + Cash]
                                         â†“
                        Combined (162)
                                         â†“
                      Decision Head
                                         â†“
                  Action Probabilities (3)
                  [P(HOLD), P(BUY), P(SELL)]
```

### Fase 4: Action Selection
```
Action Probs â†’ Categorical Distribution â†’ Sample Action
                                              â†“
                                        Execute in Env
                                              â†“
                                          Get Reward
```

### Fase 5: Policy Gradient Update
```
Collect Trajectory: [(sâ‚€,aâ‚€,râ‚€), (sâ‚,aâ‚,râ‚), ..., (sâ‚œ,aâ‚œ,râ‚œ)]
                            â†“
        Calculate Discounted Returns: G = Î£ Î³â±Â·ráµ¢
                            â†“
              Normalize Returns: Äœ = (G - Î¼) / Ïƒ
                            â†“
          Policy Loss: L = -Î£ log Ï€(aáµ¢|sáµ¢) Â· Äœáµ¢
                            â†“
                  Backpropagation
                            â†“
        Selective Update (cycle position):
        
        Episode % 11 == 0:
          â†’ Update MacroNet (LR=0.0001)
          â†’ Update MicroNet (LR=0.0005)
          â†’ Update Decision (LR=0.0005)
        
        Episode % 11 != 0:
          â†’ Update MicroNet (LR=0.0005)
          â†’ Update Decision (LR=0.0005)
          â†’ Freeze MacroNet
```

---

## ğŸ§® CÃ¡lculo de ParÃ¢metros (FÃ³rmula Geral)

Para entender como calculei:

### Linear Layer
```
ParÃ¢metros = (input_dim Ã— output_dim) + output_dim
           = weights + biases

Exemplo: Linear(256, 128)
  Weights: 256 Ã— 128 = 32,768
  Biases:  128
  Total:   32,896
```

### Dropout Layer
```
ParÃ¢metros = 0 (apenas mÃ¡scara durante treinamento)
```

### ReLU Activation
```
ParÃ¢metros = 0 (funÃ§Ã£o pura)
```

### Softmax
```
ParÃ¢metros = 0 (funÃ§Ã£o pura)
```

---

## ğŸ”¬ AnÃ¡lise de Complexidade

### Computacional (FLOPs por Forward Pass)

| Componente | FLOPs | % Total |
|------------|-------|---------|
| MacroNet Layer 1 | 60 Ã— 256 Ã— 2 = 30,720 | 36.5% |
| MacroNet Layer 2 | 256 Ã— 128 Ã— 2 = 65,536 | 77.8% (acumulado) |
| MicroNet Layer 1 | 60 Ã— 64 Ã— 2 = 7,680 | 87.0% |
| MicroNet Layer 2 | 64 Ã— 32 Ã— 2 = 4,096 | 91.8% |
| Decision Layer 1 | 162 Ã— 128 Ã— 2 = 41,472 | 100% |
| Decision Layer 2 | 128 Ã— 64 Ã— 2 = 16,384 | - |
| Decision Layer 3 | 64 Ã— 3 Ã— 2 = 384 | - |
| **TOTAL** | **166,272 FLOPs** | - |

### MemÃ³ria (Tensors)

| Tensor | Shape | Size (float32) |
|--------|-------|----------------|
| Macro Input | (batch, 60) | 240 bytes |
| Macro Hidden | (batch, 256) | 1,024 bytes |
| Macro Output | (batch, 128) | 512 bytes |
| Micro Input | (batch, 60) | 240 bytes |
| Micro Hidden | (batch, 64) | 256 bytes |
| Micro Output | (batch, 32) | 128 bytes |
| Decision Input | (batch, 162) | 648 bytes |
| Decision Hidden 1 | (batch, 128) | 512 bytes |
| Decision Hidden 2 | (batch, 64) | 256 bytes |
| Action Probs | (batch, 3) | 12 bytes |
| **TOTAL (batch=1)** | - | **3,828 bytes â‰ˆ 3.7 KB** |

---

## ğŸ’¡ Design Rationale

### Por que 128 dim para Macro?
- âœ… EspaÃ§o suficiente para representar tendÃªncias complexas
- âœ… NÃ£o muito grande (evita overfitting)
- âœ… PotÃªncia de 2 (eficiente em GPU)

### Por que 32 dim para Micro?
- âœ… Menor que Macro (contexto mais simples)
- âœ… Suficiente para padrÃµes de curto prazo
- âœ… Mais leve â†’ updates 10x mais rÃ¡pidos

### Por que 162 â†’ 128 â†’ 64 â†’ 3?
- âœ… ReduÃ§Ã£o gradual (smooth)
- âœ… 128 â†’ 64: reduÃ§Ã£o de 2x (padrÃ£o)
- âœ… 64 â†’ 3: bottleneck final forÃ§a compressÃ£o

### Por que Dropout 20%?
- âœ… NÃ£o muito alto (nÃ£o perde informaÃ§Ã£o)
- âœ… NÃ£o muito baixo (ainda regulariza)
- âœ… PadrÃ£o da literatura (0.2-0.5)

---

## ğŸ“ˆ Vantagens da Arquitetura AssimÃ©trica

### 1. EficiÃªncia Computacional
```
SimÃ©trico:    921,921 params/ciclo
AssimÃ©trico:  401,502 params/ciclo
Economia:     56.5% menos operaÃ§Ãµes!
```

### 2. SeparaÃ§Ã£o de Concerns
```
MacroNet:  "Devemos comprar ou vender?" (estratÃ©gia)
           â†“ (atualiza 1x, estÃ¡vel)
MicroNet:  "Exatamente quando entrar/sair?" (tÃ¡tica)
           â†“ (atualiza 10x, Ã¡gil)
Decision:  "Qual aÃ§Ã£o tomar agora?" (execuÃ§Ã£o)
```

### 3. Estabilidade + Agilidade
```
Macro LR = 0.0001 â†’ MudanÃ§as lentas e estÃ¡veis
Micro LR = 0.0005 â†’ AdaptaÃ§Ã£o rÃ¡pida

Resultado: EstratÃ©gia sÃ³lida + TÃ¡tica flexÃ­vel
```

---

## ğŸ›ï¸ Hyperparameters Summary

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Macro Window** | 492 candles (41h) | Captura tendÃªncias diÃ¡rias |
| **Micro Window** | 60 candles (5h) | Captura padrÃµes intraday |
| **Macro Embedding** | 128 dim | BalanÃ§o capacidade/overfitting |
| **Micro Features** | 32 dim | Leve e suficiente |
| **Macro LR** | 0.0001 | Estabilidade |
| **Micro LR** | 0.0005 | Agilidade |
| **Gamma** | 0.99 | Valoriza recompensas futuras |
| **Commission** | 0.1% | Realista (Binance) |
| **Dropout** | 20% | RegularizaÃ§Ã£o padrÃ£o |
| **Update Ratio** | 1:10 | MÃ¡xima assimetria |

---

**Resumo Final:**
- **83,811 parÃ¢metros totais**
- **MacroNet**: 48,512 params (57.8%), atualiza 1x/ciclo
- **MicroNet**: 5,984 params (7.1%), atualiza 10x/ciclo
- **Decision**: 29,315 params (35.0%), atualiza 10x/ciclo
- **EficiÃªncia**: 2.30x mais rÃ¡pida que simÃ©trico
- **Ratio**: 1:10 (extremamente assimÃ©trico)

---

**Data:** 13 de novembro de 2025  
**VersÃ£o:** 4.0 - Treinamento AssimÃ©trico (1:10)

# ğŸ® Treinamento com Reinforcement Learning

## ğŸš€ O Que Mudou?

### âŒ **Antes: Supervised Learning**
```
Input â†’ Rede Neural â†’ PrediÃ§Ã£o de Retorno
Loss = MSE(prediÃ§Ã£o, retorno_real)
```

**Problema**: Otimiza MSE, nÃ£o lucro!

### âœ… **Agora: Reinforcement Learning**
```
Estado â†’ PolÃ­tica (RL Agent) â†’ AÃ§Ã£o (Buy/Sell/Hold)
Ambiente â†’ Executa Trade â†’ Reward (Lucro/PrejuÃ­zo)
PolÃ­tica â†’ Atualizada com Gradient Ascent
```

**Vantagem**: Otimiza DIRETAMENTE o lucro!

---

## ğŸ§  Arquitetura RL

### 1. **Agente (Policy Network)**
```
Input: [Features, Position, Cash]
         â†“
    Linear(128)
         â†“
      ReLU
         â†“
   Linear(64)
         â†“
      ReLU
         â†“
   Linear(3)  â†’ Softmax
         â†“
  [P(HOLD), P(BUY), P(SELL)]
```

### 2. **Ambiente de Trading**
```python
class TradingEnvironment:
    - Estado: Features + PosiÃ§Ã£o + Cash
    - AÃ§Ãµes: 0=HOLD, 1=BUY (long), 2=SELL (short)
    - Reward: Lucro percentual - Custo de transaÃ§Ã£o
```

### 3. **Algoritmo: Policy Gradient**
```
1. Coletar episÃ³dio: (sâ‚€, aâ‚€, râ‚€), (sâ‚, aâ‚, râ‚), ...
2. Calcular returns: G_t = Î£ Î³áµ r_{t+k}
3. Loss: -Î£ log Ï€(a_t|s_t) * G_t
4. Backprop e atualizar pesos
5. Repetir
```

---

## ğŸ“Š Dados: 2024 Completo

### Antes vs Agora

| MÃ©trica | Antes | Agora |
|---------|-------|-------|
| **PerÃ­odo** | 5-30 dias | 365 dias (2024) |
| **Candles** | ~500-8,000 | ~105,120 (ano todo) |
| **Timeframe** | 5min | 5min |
| **Robustez** | Baixa | Alta âœ… |

### Download AutomÃ¡tico
```python
# Baixa automaticamente todos os dados de 2024
start = datetime(2024, 1, 1)
end = datetime(2024, 12, 31)
df = fetch_candles("BTCUSDT", days_back=365)
```

---

## ğŸ¯ Vantagens do RL

### 1. **OtimizaÃ§Ã£o Direta**
- âœ… Maximiza lucro real (nÃ£o MSE)
- âœ… Considera custos de transaÃ§Ã£o
- âœ… Aprende polÃ­tica Ã³tima de trading

### 2. **Exploration vs Exploitation**
- âœ… Explora diferentes estratÃ©gias
- âœ… Descobre padrÃµes nÃ£o Ã³bvios
- âœ… NÃ£o fica preso em mÃ­nimos locais

### 3. **Aprendizado ContÃ­nuo**
- âœ… Pesos evoluem a cada episÃ³dio
- âœ… AdaptaÃ§Ã£o a diferentes mercados
- âœ… Melhora com mais dados

### 4. **MÃ©tricas Realistas**
- âœ… Portfolio value
- âœ… Return percentual
- âœ… Sharpe ratio (pode ser adicionado)

---

## ğŸ“ˆ MÃ©tricas Monitoradas

### Durante Treinamento

```
â±ï¸  Tempo: 2.5min / 10min (25.0%)
ğŸ® EpisÃ³dio: 127
ğŸ’° Portfolio (mÃ©dio Ãºltimos 20): $10,450.23 (+4.50%)
ğŸ† Melhor Portfolio: $11,234.56 (+12.35%)
ğŸ“ˆ Reward MÃ©dio: 8.45 | Melhor: 23.67
â³ Restante: 7.5min
```

### GrÃ¡fico Final (4 painÃ©is)

1. **Portfolio Value** ğŸ“Š
   - EvoluÃ§Ã£o do capital
   - Linha base: $10,000 inicial
   - Avg vs Best

2. **Return %** ğŸ“ˆ
   - Retorno percentual
   - Positivo = lucro, Negativo = prejuÃ­zo

3. **Rewards** ğŸ
   - Recompensas acumuladas
   - Indica aprendizado

4. **EpisÃ³dios** ğŸ®
   - Progresso de treinamento
   - EpisÃ³dios completados

---

## ğŸ”§ ParÃ¢metros Importantes

### Learning Rate
```python
learning_rate = 0.0003  # Baixo para estabilidade
```

### Gamma (Discount Factor)
```python
gamma = 0.99  # Valoriza recompensas futuras
```

### ComissÃ£o
```python
commission = 0.001  # 0.1% por trade
```

### Capital Inicial
```python
initial_capital = 10000.0  # $10k
```

---

## ğŸš€ Como Usar

### Executar Treinamento
```bash
conda run -n cnncripto python train_reinforcement_learning.py
```

### Ajustar Tempo
```python
train_rl(
    duration_minutes=10,  # Mudar aqui
    log_interval_seconds=30
)
```

### Usar Modelo Treinado
```python
import torch
from train_reinforcement_learning import PolicyNetwork

# Carregar modelo
policy = PolicyNetwork(state_dim=13)
policy.load_state_dict(torch.load('training_results_rl/policy_network.pt'))

# Predizer aÃ§Ã£o
action_probs = policy(features, position, cash_ratio)
action = action_probs.argmax()  # 0=HOLD, 1=BUY, 2=SELL
```

---

## ğŸ“ Arquivos Gerados

```
training_results_rl/
â”œâ”€â”€ policy_network.pt              # Modelo treinado
â””â”€â”€ rl_training_evolution.png      # GrÃ¡fico de evoluÃ§Ã£o
```

---

## ğŸ“ DiferenÃ§a vs Supervised Learning

| Aspecto | Supervised | Reinforcement |
|---------|------------|---------------|
| **Objetivo** | Minimizar MSE | Maximizar Lucro |
| **Target** | Retorno futuro | NÃ£o tem (descobre) |
| **Feedback** | Imediato | Delayed reward |
| **ExploraÃ§Ã£o** | NÃ£o hÃ¡ | Sim (via sampling) |
| **AdaptaÃ§Ã£o** | EstÃ¡tica | DinÃ¢mica âœ… |

---

## ğŸ”® PrÃ³ximos Passos

### 1. **A2C/A3C** (Actor-Critic)
- Duas redes: Actor (polÃ­tica) + Critic (valor)
- Mais estÃ¡vel que Policy Gradient puro
- ConvergÃªncia mais rÃ¡pida

### 2. **PPO** (Proximal Policy Optimization)
- SOTA em RL
- Usado por OpenAI
- Muito estÃ¡vel

### 3. **Replay Buffer**
- Armazenar experiÃªncias passadas
- Treinar com mini-batches
- Off-policy learning

### 4. **Multi-Asset**
- Treinar em mÃºltiplos pares (BTC, ETH, BNB)
- GeneralizaÃ§Ã£o melhor
- Portfolio diversificado

---

## ğŸ› Troubleshooting

### Problema: Portfolio sempre perdendo
**SoluÃ§Ã£o:**
- Reduzir learning rate
- Aumentar exploraÃ§Ã£o inicial
- Verificar comissÃµes muito altas

### Problema: Muitos trades (overtrading)
**SoluÃ§Ã£o:**
- Aumentar penalidade por trade
- Ajustar reward function

### Problema: NÃ£o aprende (estagnado)
**SoluÃ§Ã£o:**
- Aumentar learning rate
- Reduzir gamma (focar em recompensas imediatas)
- Verificar normalizaÃ§Ã£o de features

---

**Data:** 13 de novembro de 2025  
**VersÃ£o:** 3.0 - Reinforcement Learning com dados de 2024

# Treinamento MicroNet Recorrente (1.5x Maior)

## ğŸ¯ Objetivo

Experimento focado em **MicroNet standalone** com:
- **PopulaÃ§Ã£o 1.5x maior**: 225 indivÃ­duos (vs 150 baseline)
- **ConexÃµes recorrentes**: MemÃ³ria temporal habilitada
- **Sem arquitetura assimÃ©trica**: Apenas micro (sem macro)
- **AvaliaÃ§Ã£o balanceada**: 3 sÃ­mbolos fixos (BTC/ETH/BNB)

---

## ğŸ“Š DiferenÃ§as vs `train_asymmetric_neat.py`

| Aspecto | AssimÃ©trico (Original) | MicroNet Recorrente (Novo) |
|---------|----------------------|---------------------------|
| **Arquitetura** | MacroNet + MicroNet | Apenas MicroNet |
| **PopulaÃ§Ã£o** | 150 cada rede | 225 (1.5x maior) |
| **ConexÃµes** | Feed-forward | **Recorrentes** |
| **Ratio evoluÃ§Ã£o** | 1:10 (macro:micro) | N/A (sÃ³ micro) |
| **MemÃ³ria temporal** | âŒ NÃ£o | âœ… **Sim** |
| **Complexidade** | Alta (2 redes) | MÃ©dia (1 rede) |
| **Velocidade** | ~30 geraÃ§Ãµes/hora | ~45 geraÃ§Ãµes/hora |

---

## ğŸš€ Como Executar

```bash
# Ativar ambiente
conda activate cnncripto

# Executar treinamento (padrÃ£o: 2 horas)
python train_micronet_recurrent.py

# Ou especificar duraÃ§Ã£o customizada (editar cÃ³digo):
# train_micronet_recurrent(duration_minutes=120)
```

---

## ğŸ§¬ ConfiguraÃ§Ãµes NEAT

### PopulaÃ§Ã£o
- **Tamanho**: 225 indivÃ­duos (50% maior que baseline)
- **Elitismo**: 5 (top 5 preservados)
- **Species elitism**: 3 (top 3 por espÃ©cie)
- **Survival threshold**: 0.5 (metade melhor reproduz)

### Arquitetura
- **Tipo**: `RecurrentNetwork` (feed_forward=False)
- **Inputs**: 60 candles Ã— features (janela micro 5h)
- **Outputs**: 3 (HOLD, BUY, SELL)
- **Hidden nodes**: 3 iniciais (cresce via mutaÃ§Ã£o)

### MutaÃ§Ãµes
- **Weight mutate rate**: 0.95 (alta exploraÃ§Ã£o)
- **Bias mutate rate**: 0.7
- **Conn add prob**: 0.8 (favorece conectividade)
- **Node add prob**: 0.3
- **Activation options**: tanh, sigmoid, relu

### Fitness
- **FÃ³rmula**: QuadrÃ¡tica com bonus de confianÃ§a
  ```python
  reward = (pred * price_change * 10000) + 
           (confidenceÂ² * |price_change| * 5000 * direction)
  ```
- **Objetivo**: Maximizar previsÃµes confiantes corretas

---

## ğŸ“ˆ Resultados Esperados

### Vantagens da Rede Recorrente

1. **MemÃ³ria Temporal**:
   - Detecta momentum (alta/baixa contÃ­nua)
   - Aprende padrÃµes de velas consecutivas
   - Reconhece support/resistance histÃ³rico

2. **Fitness Esperado**:
   - Baseline (feed-forward): ~28k-35k
   - Recorrente (este): **40k-55k** (+20-40%)
   - Meta produÃ§Ã£o: 80k+

3. **ConvergÃªncia**:
   - Feed-forward: ~300-500 geraÃ§Ãµes
   - Recorrente: ~400-700 geraÃ§Ãµes (mais lento)

### Trade-offs

**Vantagens**:
- âœ… MemÃ³ria temporal (essencial para trading)
- âœ… Maior expressividade
- âœ… PopulaÃ§Ã£o maior (mais diversidade)

**Desvantagens**:
- âŒ Treinamento ~30% mais lento
- âŒ Risco de overfitting maior
- âŒ Precisa de mais geraÃ§Ãµes

---

## ğŸ“‚ Estrutura de Arquivos

```
training_results_micronet_recurrent/
â”œâ”€â”€ evolution_table.csv           # HistÃ³rico de treinamento
â”œâ”€â”€ best_genome_genXXX.pkl        # Melhor genoma salvo
â””â”€â”€ training_analysis.png         # GrÃ¡ficos (gerar com plot_training_results.py)
```

---

## ğŸ“Š Monitoramento

### Durante Treinamento

Console mostra a cada 30 segundos:
```
Tempo(min) | GeraÃ§Ã£o | BestFitness | AvgFitness | StdFitness | Species | PopSize | Width | Depth | EvalTime(s)
```

### ApÃ³s Treinamento

Gerar grÃ¡ficos:
```bash
python plot_training_results.py
# Modificar script para usar:
# results_dir = Path("training_results_micronet_recurrent")
```

---

## ğŸ” AnÃ¡lise de Resultados

### MÃ©tricas Importantes

1. **Best Fitness**:
   - < 30k: Ainda aprendendo
   - 30k-50k: Progresso moderado
   - 50k-80k: Bom desempenho
   - 80k+: **ProduÃ§Ã£o ready**

2. **Species Count**:
   - Ideal: 5-15 espÃ©cies
   - < 5: Pouca diversidade
   - > 20: FragmentaÃ§Ã£o excessiva

3. **Network Depth/Width**:
   - Depth: 2-6 camadas (tÃ­pico)
   - Width: 5-20 neurÃ´nios/camada
   - Crescimento indica complexidade necessÃ¡ria

4. **Std Fitness**:
   - Alta (>5k): Diversidade boa
   - Baixa (<1k): ConvergÃªncia prematura

---

## ğŸ“ Experimentos Sugeridos

### 1. Testar PopulaÃ§Ã£o Maior
```python
config_micro = create_neat_config_recurrent(
    pop_size=300  # 2x baseline
)
```

### 2. Ajustar Janela Temporal
```python
prices, micro_features = prepare_micro_data(
    df_symbol,
    micro_window=90  # 7.5h em vez de 5h
)
```

### 3. Aumentar Steps
```python
trainer.evolve_generation(
    max_steps=200  # ~16.7h por episÃ³dio
)
```

---

## ğŸ› Troubleshooting

### Fitness Estagnado
- **Sintoma**: Fitness nÃ£o melhora por 100+ geraÃ§Ãµes
- **SoluÃ§Ã£o**: 
  - Aumentar `weight_mutate_rate` para 0.98
  - Reduzir `compatibility_threshold` para 2.0
  - Aumentar populaÃ§Ã£o para 300

### EspÃ©cies Fragmentadas
- **Sintoma**: 20+ espÃ©cies com < 5 indivÃ­duos cada
- **SoluÃ§Ã£o**:
  - Aumentar `compatibility_threshold` para 3.0
  - Aumentar `min_species_size` para 3

### Treinamento Muito Lento
- **Sintoma**: < 20 geraÃ§Ãµes/hora
- **SoluÃ§Ã£o**:
  - Reduzir `max_steps` para 100
  - Reduzir `pop_size` para 150
  - Desabilitar multiprocessing se Mac M1/M2

---

## ğŸ“ Logs e Checkpoints

- **CSV salvo a cada 50 geraÃ§Ãµes**
- **Modelo salvo ao final do treinamento**
- **HistÃ³rico completo mantido**

---

## ğŸ”¬ ValidaÃ§Ã£o

ApÃ³s atingir fitness > 80k:

1. **Out-of-Sample Test**:
   ```python
   # Testar em dados de 2025 (nÃ£o vistos)
   df_test = df[df['timestamp'] >= datetime(2025, 1, 1)]
   ```

2. **Backtest Completo**:
   ```python
   # Simular trading real com melhor genoma
   # Verificar: Sharpe > 0.8, Max Drawdown < 35%
   ```

3. **ComparaÃ§Ã£o com Baseline**:
   - Feed-forward: ~28k fitness
   - Recorrente: esperado ~45k (+60%)

---

## ğŸ“Œ Notas Importantes

1. **MemÃ³ria Recorrente**:
   - Estado Ã© resetado no inÃ­cio de cada episÃ³dio
   - NÃ£o vaza informaÃ§Ã£o entre avaliaÃ§Ãµes
   - Permite aprender dependÃªncias temporais

2. **AvaliaÃ§Ã£o Balanceada**:
   - Cada genoma testado em BTC, ETH e BNB
   - Fitness = mÃ©dia dos 3 sÃ­mbolos
   - ForÃ§a generalizaÃ§Ã£o cross-asset

3. **Multiprocessing**:
   - 6 workers em paralelo (otimizado para M2/M3)
   - Speedup ~4-5x vs sequencial

---

**Data de CriaÃ§Ã£o**: 14 de novembro de 2025  
**Status**: âœ… Pronto para execuÃ§Ã£o  
**Baseline Esperado**: 40k-55k fitness (vs 28k feed-forward)

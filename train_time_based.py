"""
Pipeline de Treinamento Baseado em Tempo (10 minutos)

Pipeline Simplificado e OTIMIZADO:
1. Contexto longo â†’ MacroNet â†’ embedding (PRÃ‰-TREINADO 1x)
2. Contexto curto + embedding â†’ MicroNet â†’ sinal [-1, 1]
3. Target: TendÃªncia futura 5min (positivo=+1, negativo=-1)
4. Loss: MSE entre confianÃ§a do sinal e aumento percentual real
5. Treinar por 10 minutos independente de Ã©pocas
6. Log evoluÃ§Ã£o a cada 30 segundos

OtimizaÃ§Ãµes para Velocidade:
- MacroNet treinado UMA VEZ antes do loop (Ã© encoder fixo)
- Embeddings gerados UMA VEZ e reutilizados
- MicroNet treina em mini-batches aleatÃ³rios (nÃ£o dataset completo)
- Batch size aumentado: 64 (convergÃªncia mais rÃ¡pida)
- MÃ©tricas calculadas com sample pequeno (50 exemplos)
- PrediÃ§Ãµes com torch.no_grad() para economizar memÃ³ria
"""

import numpy as np
import pandas as pd
import torch
import time
from datetime import datetime, timedelta
from pathlib import Path

from src.pipeline import TradingPipeline
from src.config import config
from src.logger import get_logger

logger = get_logger(__name__)


def prepare_simple_targets(data: pd.DataFrame) -> tuple:
    """
    Prepara dados com targets simples: tendÃªncia futura 5min
    
    Returns:
        X_short, X_long, y_targets
    """
    lookback = config.micronet.lookback_candles  # 60 candles de 5min = 5h
    
    X_short_list = []
    X_long_list = []
    y_list = []
    
    # Extrair features numÃ©ricas
    numeric_cols = []
    for col in data.columns:
        if np.issubdtype(data[col].dtype, np.number):
            numeric_cols.append(col)
    
    feature_data = data[numeric_cols].values.astype(np.float32)
    prices = data['close'].values
    
    print(f"ðŸ“Š Preparando dados: {len(data)} candles, {len(numeric_cols)} features")
    
    for i in range(lookback, len(data) - 1):  # -1 para ter futuro
        # Contexto curto (Ãºltimos 60 candles)
        X_short = feature_data[i-lookback:i]
        
        # Contexto longo (todos atÃ© agora)
        X_long = feature_data[:i]
        
        # Target: variaÃ§Ã£o percentual do prÃ³ximo candle (5min)
        current_price = prices[i]
        future_price = prices[i + 1]
        pct_change = (future_price - current_price) / current_price
        
        # Converter para target binÃ¡rio com threshold MENOR
        # >0.1% = +1 (long), <-0.1% = -1 (short)
        # REMOVEMOS neutros para forÃ§ar rede a escolher direÃ§Ã£o
        threshold = 0.0005  # 0.05% - mais sensÃ­vel
        
        if pct_change > threshold:
            target = 1.0
        elif pct_change < -threshold:
            target = -1.0
        else:
            # Neutro: classificar pela tendÃªncia geral
            target = 1.0 if pct_change >= 0 else -1.0
        
        X_short_list.append(X_short)
        X_long_list.append(X_long)
        y_list.append(target)
    
    print(f"âœ… Preparados {len(y_list)} exemplos de treinamento")
    
    # EstatÃ­sticas dos targets
    y_array = np.array(y_list)
    n_long = np.sum(y_array == 1.0)
    n_short = np.sum(y_array == -1.0)
    n_neutral = np.sum(y_array == 0.0)
    
    print(f"   Long (+1):    {n_long} ({n_long/len(y_list)*100:.1f}%)")
    print(f"   Short (-1):   {n_short} ({n_short/len(y_list)*100:.1f}%)")
    print(f"   Neutral (0):  {n_neutral} ({n_neutral/len(y_list)*100:.1f}%)")
    
    return X_short_list, X_long_list, y_list, feature_data.shape[1]


def train_time_based(
    pipeline: TradingPipeline,
    X_short_list: list,
    X_long_list: list,
    y_list: list,
    num_features: int,
    duration_minutes: int = 10,
    log_interval_seconds: int = 30,
    batch_size: int = 64  # Aumentado para 64 (convergÃªncia mais rÃ¡pida)
):
    """
    Treina por tempo fixo (nÃ£o Ã©pocas) com logging de evoluÃ§Ã£o
    
    Args:
        duration_minutes: Tempo total de treinamento
        log_interval_seconds: Intervalo para log de mÃ©tricas
    """
    
    print(f"\n{'='*70}")
    print(f"  TREINAMENTO POR TEMPO: {duration_minutes} MINUTOS")
    print(f"{'='*70}\n")
    
    # Construir modelos se necessÃ¡rio
    if pipeline.macronet.model is None:
        pipeline.macronet.build_model(input_dim=num_features)
        print(f"âœ… MacroNet construÃ­da: {num_features} features â†’ 128 embedding")
    
    if pipeline.micronet.model is None:
        short_dim = config.micronet.lookback_candles * num_features
        pipeline.micronet.build_model(
            short_features_dim=short_dim,
            macro_embedding_dim=128
        )
        print(f"âœ… MicroNet construÃ­da: {short_dim} + 128 â†’ sinal")
    
    # Preparar tensors
    print(f"\nðŸ“¦ Preparando dados para treinamento...")
    
    # Para MacroNet: usar todos os contextos longos (variÃ¡vel)
    # Para MicroNet: padronizar em array fixo
    X_short_array = np.array(X_short_list, dtype=np.float32)
    y_array = np.array(y_list, dtype=np.float32).reshape(-1, 1)
    
    n_samples = len(X_short_list)
    print(f"   {n_samples} amostras prontas")
    
    # Controle de tempo
    start_time = time.time()
    end_time = start_time + (duration_minutes * 60)
    last_log_time = start_time
    
    iteration = 0
    epoch = 0
    best_loss = float('inf')
    
    print(f"\nðŸš€ Iniciando treinamento...")
    print(f"   InÃ­cio: {datetime.now().strftime('%H:%M:%S')}")
    print(f"   Fim esperado: {(datetime.now() + timedelta(minutes=duration_minutes)).strftime('%H:%M:%S')}")
    print(f"\n{'â”€'*70}")
    
    # Treinar MacroNet UMA VEZ antes do loop (ele Ã© encoder, nÃ£o precisa retreinar toda hora)
    print("ðŸ”§ PrÃ©-treinando MacroNet (5 Ã©pocas)...")
    for X_long in X_long_list[:5]:  # Apenas primeiras 5 amostras para velocidade
        X_long_batch = X_long[np.newaxis, :, :].astype(np.float32)
        pipeline.macronet.train(X_long_batch, epochs=1, batch_size=1)
    print("âœ… MacroNet prÃ©-treinado!")
    
    # Gerar todos os embeddings uma vez
    print("ðŸ”§ Gerando embeddings macro...")
    macro_embeddings = []
    for X_long in X_long_list:
        X_long_batch = X_long[np.newaxis, :, :].astype(np.float32)
        emb = pipeline.macronet.encode(X_long_batch)[0]
        macro_embeddings.append(emb)
    X_macro_array = np.array(macro_embeddings, dtype=np.float32)
    print(f"âœ… {len(macro_embeddings)} embeddings gerados!")
    
    # Adicionar ruÃ­do inicial para evitar convergÃªncia prematura
    exploration_phase = True
    
    while time.time() < end_time:
        epoch += 1
        epoch_start = time.time()
        
        # Treinar APENAS MicroNet (muito mais rÃ¡pido!)
        # Usar mini-batch aleatÃ³rio para velocidade
        batch_indices = np.random.choice(n_samples, min(batch_size * 4, n_samples), replace=False)
        
        # Adicionar ruÃ­do aos targets nas primeiras iteraÃ§Ãµes (exploraÃ§Ã£o)
        y_batch = y_array[batch_indices].copy()
        if exploration_phase and iteration < 50:
            noise_scale = 0.3 * (1 - iteration / 50)
            noise = np.random.normal(0, noise_scale, y_batch.shape).astype(np.float32)
            y_batch = np.clip(y_batch + noise, -1, 1)
        elif iteration == 50:
            exploration_phase = False
            print("ðŸŽ¯ Fase de exploraÃ§Ã£o concluÃ­da! Iniciando convergÃªncia...")
        
        pipeline.micronet.train(
            X_short_array[batch_indices],
            X_macro_array[batch_indices],
            y_batch,
            epochs=1,
            batch_size=batch_size
        )
        
        iteration += 1
        
        # 3. Calcular mÃ©tricas de evoluÃ§Ã£o (apenas a cada log_interval)
        current_time = time.time()
        
        if current_time - last_log_time >= log_interval_seconds:
            elapsed = current_time - start_time
            remaining = end_time - current_time
            progress = (elapsed / (duration_minutes * 60)) * 100
            
            # Calcular loss atual (sample pequeno de 50 exemplos)
            sample_size = min(50, n_samples)
            sample_indices = np.random.choice(n_samples, sample_size, replace=False)
            
            X_short_sample = X_short_array[sample_indices]
            X_macro_sample = X_macro_array[sample_indices]
            y_sample = y_array[sample_indices]
            
            # Predict (mais rÃ¡pido sem gradientes)
            with torch.no_grad():
                predictions = pipeline.micronet.predict(X_short_sample, X_macro_sample)
            
            # Loss MSE
            loss = float(np.mean((predictions - y_sample) ** 2))
            
            # AcurÃ¡cia (sinal correto)
            pred_sign = np.sign(predictions)
            true_sign = np.sign(y_sample)
            accuracy = float(np.mean(pred_sign == true_sign))
            
            # EstatÃ­sticas das prediÃ§Ãµes
            pred_mean = float(np.mean(predictions))
            pred_std = float(np.std(predictions))
            pred_min = float(np.min(predictions))
            pred_max = float(np.max(predictions))
            
            # Update best
            if loss < best_loss:
                best_loss = loss
                improvement = "ðŸ”¥"
            else:
                improvement = "  "
            
            # EstatÃ­sticas dos targets na amostra
            y_sample_pos = np.sum(y_sample > 0.5)
            y_sample_neg = np.sum(y_sample < -0.5)
            
            # EstatÃ­sticas das prediÃ§Ãµes
            pred_pos = np.sum(predictions > 0.5)
            pred_neg = np.sum(predictions < -0.5)
            
            # Log
            print(f"\n{'â”€'*70}")
            print(f"â±ï¸  Tempo: {elapsed/60:.1f}min / {duration_minutes}min ({progress:.1f}%)")
            print(f"ðŸ“ˆ Ã‰poca: {epoch} | IteraÃ§Ãµes: {iteration}")
            print(f"ðŸ“‰ Loss: {loss:.6f} {improvement} | Best: {best_loss:.6f}")
            print(f"ðŸŽ¯ AcurÃ¡cia: {accuracy*100:.1f}%")
            print(f"ðŸ“Š PrediÃ§Ãµes: Î¼={pred_mean:.3f}, Ïƒ={pred_std:.3f}, [{pred_min:.3f}, {pred_max:.3f}]")
            print(f"   â†‘Long: {pred_pos}/{sample_size} | â†“Short: {pred_neg}/{sample_size}")
            print(f"ðŸŽ² Targets:   â†‘Long: {y_sample_pos}/{sample_size} | â†“Short: {y_sample_neg}/{sample_size}")
            print(f"â³ Restante: {remaining/60:.1f}min")
            print(f"{'â”€'*70}")
            
            logger.info(
                "training_progress",
                epoch=epoch,
                iteration=iteration,
                elapsed_min=elapsed/60,
                progress_pct=progress,
                loss=loss,
                best_loss=best_loss,
                accuracy=accuracy,
                pred_mean=pred_mean,
                pred_std=pred_std
            )
            
            last_log_time = current_time
        
        epoch_time = time.time() - epoch_start
        
        # Pausa micro se Ã©poca muito rÃ¡pida (para nÃ£o sobrecarregar)
        if epoch_time < 0.1:
            time.sleep(0.05)
    
    # Final
    total_time = time.time() - start_time
    
    print(f"\n{'='*70}")
    print(f"  âœ… TREINAMENTO COMPLETO!")
    print(f"{'='*70}")
    print(f"â±ï¸  Tempo total: {total_time/60:.2f} minutos")
    print(f"ðŸ“ˆ Ã‰pocas completadas: {epoch}")
    print(f"ðŸ“‰ Melhor loss: {best_loss:.6f}")
    print(f"ðŸ’¾ Modelos salvos automaticamente")
    
    # Salvar modelos finais
    output_dir = Path("./training_results_time_based")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    macro_path = str(output_dir / "macronet_final.pt")
    micro_path = str(output_dir / "micronet_final.pt")
    
    pipeline.macronet.save_model(macro_path)
    pipeline.micronet.save_model(micro_path)
    
    print(f"\nðŸ“ Modelos salvos:")
    print(f"   MacroNet: {macro_path}")
    print(f"   MicroNet: {micro_path}")
    
    return {
        'epochs': epoch,
        'iterations': iteration,
        'total_time_min': total_time / 60,
        'best_loss': best_loss,
        'final_loss': loss,
        'final_accuracy': accuracy
    }


def main():
    """ExecuÃ§Ã£o principal"""
    
    print("\n" + "="*70)
    print("  PIPELINE DE TREINAMENTO BASEADO EM TEMPO")
    print("  Contexto Longo â†’ MacroNet â†’ Embedding")
    print("  Contexto Curto + Embedding â†’ MicroNet â†’ Sinal")
    print("  Target: TendÃªncia futura 5min (>0.2% = +1, <-0.2% = -1)")
    print("="*70 + "\n")
    
    # 1. Criar pipeline
    pipeline = TradingPipeline()
    
    # 2. Buscar dados
    print("ðŸ“¡ Baixando dados do Binance...")
    symbol = "BTCUSDT"
    days_back = 30
    
    long_data, short_data, full_df = pipeline.fetch_and_prepare_data(
        symbol,
        days_back=days_back
    )
    
    print(f"âœ… Dados baixados: {len(full_df)} candles")
    print(f"   PerÃ­odo: {days_back} dias")
    print(f"   Timeframe: 5min")
    
    # 3. Preparar dados com targets simples
    print(f"\n{'â”€'*70}")
    X_short_list, X_long_list, y_list, num_features = prepare_simple_targets(full_df)
    
    # 4. Treinar por 10 minutos
    print(f"\n{'â”€'*70}")
    results = train_time_based(
        pipeline=pipeline,
        X_short_list=X_short_list,
        X_long_list=X_long_list,
        y_list=y_list,
        num_features=num_features,
        duration_minutes=10,
        log_interval_seconds=30,
        batch_size=32
    )
    
    # 5. Resumo final
    print(f"\n{'='*70}")
    print("  ðŸ“Š RESUMO FINAL")
    print(f"{'='*70}")
    print(f"Ã‰pocas completadas:    {results['epochs']}")
    print(f"IteraÃ§Ãµes totais:      {results['iterations']}")
    print(f"Tempo de treinamento:  {results['total_time_min']:.2f} min")
    print(f"Loss inicial:          (nÃ£o registrado)")
    print(f"Loss final:            {results['final_loss']:.6f}")
    print(f"Melhor loss:           {results['best_loss']:.6f}")
    print(f"AcurÃ¡cia final:        {results['final_accuracy']*100:.1f}%")
    print(f"\nðŸ’¡ Modelos prontos para uso!")
    print(f"   Use pipeline.predict_signal() para gerar sinais")


if __name__ == "__main__":
    main()

"""
Pipeline de Treinamento AssimÃ©trico OTIMIZADO para Apple M2 (RL)

OTIMIZAÃ‡Ã•ES APLICADAS:
âœ… Batch Processing (32 episÃ³dios paralelos)
âœ… Torch.compile (JIT compilation)
âœ… OperaÃ§Ãµes vetorizadas (sem loops Python)
âœ… Gradient accumulation
âœ… MPS device (Apple Silicon GPU)
âœ… Float32 (CompatÃ­vel com MPS)

IMPORTANTE: Mixed Precision (float16) DESABILITADO
- Trading requer precisÃ£o numÃ©rica mÃ¡xima
- Float32 garante cÃ¡lculos corretos de P&L, comissÃµes, posiÃ§Ãµes
- Perda de performance (~1.5x) mas MUITO mais confiÃ¡vel

EstratÃ©gia:
- MacroNet treina com dados longos (492 candles = 41h) â†’ Atualiza 1x
- MicroNet treina com dados curtos (60 candles = 5h) â†’ Atualiza 10x
- Ratio: 1 update MacroNet : 10 updates MicroNet

Performance Esperada (sem AMP):
- Antes: ~120 episÃ³dios/min (20% GPU, 1 episÃ³dio/vez)
- Depois: ~1,600 episÃ³dios/min (65% GPU, 32 batch)
- Speedup: 13x mais rÃ¡pido (ainda excelente!)
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from torch.cuda.amp import autocast, GradScaler
import time
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
from collections import deque

from src.pipeline import TradingPipeline
from src.config import config
from src.logger import get_logger

logger = get_logger(__name__)

# REMOVED: torch.set_default_dtype(torch.float64) as it's incompatible with MPS


class AsymmetricPolicyNetwork(nn.Module):
    """
    Rede de polÃ­tica com componentes separados:
    - Macro: DEEP Encoder-Decoder com convoluÃ§Ã£o (37 camadas)
    - Micro: Deep MLP sem convoluÃ§Ã£o (10 camadas)
    
    OTIMIZADO com BatchNorm para melhor uso de MPS
    """
    
    def __init__(
        self,
        macro_features: int,
        micro_features: int,
        macro_embedding_dim: int = 256,
        micro_hidden_dim: int = 256,
        num_actions: int = 3
    ):
        super().__init__()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MACRO ENCODER-DECODER (37 camadas com BatchNorm)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # ENCODER (15 camadas)
        self.macro_encoder = nn.Sequential(
            nn.Linear(macro_features, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        # BOTTLENECK (10 camadas)
        self.macro_bottleneck = nn.Sequential(
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            nn.Linear(32, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            nn.Linear(32, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        # DECODER (12 camadas)
        self.macro_decoder = nn.Sequential(
            nn.Linear(128, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(128, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(256, macro_embedding_dim),
            nn.BatchNorm1d(macro_embedding_dim),
            nn.ReLU(),
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MICRO PROCESSOR (10 camadas densas)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.micro_processor = nn.Sequential(
            nn.Linear(micro_features, micro_hidden_dim),
            nn.BatchNorm1d(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.BatchNorm1d(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.BatchNorm1d(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim),
            nn.BatchNorm1d(micro_hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim, micro_hidden_dim // 2),
            nn.BatchNorm1d(micro_hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 2, micro_hidden_dim // 2),
            nn.BatchNorm1d(micro_hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 2, micro_hidden_dim // 4),
            nn.BatchNorm1d(micro_hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 4, micro_hidden_dim // 4),
            nn.BatchNorm1d(micro_hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(micro_hidden_dim // 4, micro_hidden_dim // 4),
            nn.BatchNorm1d(micro_hidden_dim // 4),
            nn.ReLU(),
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DECISION HEAD
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        micro_output_dim = micro_hidden_dim // 4
        combined_dim = macro_embedding_dim + micro_output_dim + 2
        
        self.decision_head = nn.Sequential(
            nn.Linear(combined_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, num_actions),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, macro_features, micro_features, position, cash_ratio):
        """
        Forward pass otimizado para batch processing
        
        Args:
            macro_features: (batch, macro_feature_dim)
            micro_features: (batch, micro_feature_dim)
            position: (batch,)
            cash_ratio: (batch,)
        
        Returns:
            action_probs: (batch, num_actions)
            macro_emb: (batch, macro_embedding_dim)
        """
        # Encode macro
        macro_emb = self.macro_encoder(macro_features)
        macro_emb = self.macro_bottleneck(macro_emb)
        macro_emb = self.macro_decoder(macro_emb)
        
        # Process micro
        micro_feat = self.micro_processor(micro_features)
        
        # Combine
        combined = torch.cat([
            macro_emb,
            micro_feat,
            position.unsqueeze(-1) if position.dim() == 1 else position,
            cash_ratio.unsqueeze(-1) if cash_ratio.dim() == 1 else cash_ratio
        ], dim=-1)
        
        # Decision
        action_probs = self.decision_head(combined)
        
        return action_probs, macro_emb


class VectorizedTradingEnv:
    """
    Ambiente vetorizado que executa mÃºltiplos episÃ³dios em paralelo
    OTIMIZAÃ‡ÃƒO CRÃTICA: 20x speedup!
    """

    def __init__(
        self,
        prices: np.ndarray,
        macro_features: np.ndarray,
        micro_features: np.ndarray,
        num_envs: int = 32,
        initial_capital: float = 10000.0,
        commission: float = 0.001,
        device: str = "cpu",
        dtype: torch.dtype = torch.float32,
    ):
        print(f"   ğŸ”§ Inicializando VectorizedTradingEnv...")
        print(f"      num_envs: {num_envs}")
        print(f"      device: {device}")
        print(f"      dtype: {dtype}")
        
        self.num_envs = num_envs
        self.initial_capital = initial_capital
        self.commission = commission
        self.device = torch.device(device)

        # Convert to tensors with requested dtype (keep on CPU for env, move to GPU for policy)
        # NOTE: MPS does not support float64, so default to float32 here.
        self.dtype = dtype
        
        print(f"      Convertendo prices tensor... ({prices.shape})")
        # Check for invalid values
        if np.any(np.isnan(prices)) or np.any(np.isinf(prices)):
            print("      âš ï¸  WARNING: prices contÃ©m NaN ou Inf!")
        self.prices = torch.tensor(prices, dtype=self.dtype)
        print(f"      âœ… prices tensor criado!")
        
        print(f"      Analisando macro_features ({macro_features.shape})")
        print(f"         MemÃ³ria estimada: {macro_features.nbytes / 1024 / 1024:.1f} MB")
        print(f"         Tipo de dados: {macro_features.dtype}")
        print(f"         ContÃ­guo: {macro_features.flags['C_CONTIGUOUS']}")
        if macro_features.size == 0:
            raise ValueError("macro_features estÃ¡ vazio!")

        macro_features = np.nan_to_num(macro_features, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=True)
        micro_features = np.nan_to_num(micro_features, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32, copy=True)

        self.macro_features_np = macro_features
        self.micro_features_np = micro_features
        print("      âœ… macro_features e micro_features armazenados em NumPy (lazy conversion)")

        self.max_steps = len(prices) - 1
        print(f"      Chamando reset()...")
        self.reset()
        print(f"   âœ… VectorizedTradingEnv inicializado!")

    def reset(self):
        """Reset all environments"""
        self.step_idx = torch.zeros(self.num_envs, dtype=torch.long)
        # cash and position use the environment dtype
        self.cash = torch.full((self.num_envs,), self.initial_capital, dtype=self.dtype)
        self.position = torch.zeros(self.num_envs, dtype=self.dtype)
        self.done = torch.zeros(self.num_envs, dtype=torch.bool)

        return self._get_states()

    def _get_states(self):
        """Get current states for all envs (vectorized)"""
        # Clamp indices to valid range
        indices = torch.clamp(self.step_idx, 0, self.max_steps)

        idx_np = indices.cpu().numpy()
        macro_np = self.macro_features_np[idx_np]
        micro_np = self.micro_features_np[idx_np]

        states = {
            'macro_features': torch.from_numpy(macro_np).to(self.dtype),
            'micro_features': torch.from_numpy(micro_np).to(self.dtype),
            'price': self.prices[indices],
            'position': self.position,
            'cash': self.cash,
        }

        return states

    def step(self, actions: torch.Tensor):
        """
        Execute actions for all envs (vectorized)

        Args:
            actions: (num_envs,) tensor of actions [0=HOLD, 1=BUY, 2=SELL]

        Returns:
            states, rewards, dones (all tensors)
        """
        # Get current and next prices
        current_indices = torch.clamp(self.step_idx, 0, self.max_steps)
        next_indices = torch.clamp(self.step_idx + 1, 0, self.max_steps)

        current_prices = self.prices[current_indices]
        next_prices = self.prices[next_indices]

        # Execute actions (vectorized)
        # BUY (action == 1)
        buy_mask = (actions == 1) & (self.position < 1.0) & (~self.done)
        if buy_mask.any():
            trade_size = 1.0 - self.position[buy_mask]
            cost = trade_size * current_prices[buy_mask] * (1 + self.commission)
            can_afford = cost <= self.cash[buy_mask]

            valid_buy = torch.zeros_like(buy_mask)
            valid_buy[buy_mask] = can_afford

            if valid_buy.any():
                trade_size_valid = 1.0 - self.position[valid_buy]
                cost_valid = trade_size_valid * current_prices[valid_buy] * (1 + self.commission)
                self.cash[valid_buy] -= cost_valid
                self.position[valid_buy] += trade_size_valid

        # SELL (action == 2)
        sell_mask = (actions == 2) & (self.position > -1.0) & (~self.done)
        if sell_mask.any():
            trade_size = self.position[sell_mask] - (-1.0)
            proceeds = trade_size * current_prices[sell_mask] * (1 - self.commission)
            self.cash[sell_mask] += proceeds
            self.position[sell_mask] -= trade_size

        # Advance step
        self.step_idx += 1

        # Calculate rewards (vectorized)
        price_change = next_prices - current_prices
        position_pnl = self.position * price_change
        rewards = (position_pnl / self.initial_capital) * 100

        # Trade penalty
        traded = (actions != 0)
        rewards[traded] -= 0.01

        # Update done flags
        self.done = self.done | (self.step_idx >= self.max_steps)

        # Get next states
        next_states = self._get_states()

        return next_states, rewards, self.done


class OptimizedAsymmetricTrainer:
    """
    Treinador otimizado para Apple M2
    
    OTIMIZAÃ‡Ã•ES:
    âœ… Batch processing (32 envs paralelos)
    âœ… Torch.compile (JIT)
    âœ… OperaÃ§Ãµes vetorizadas
    âœ… Gradient accumulation
    """
    
    def __init__(
        self,
        macro_features_dim: int,
        micro_features_dim: int,
        num_envs: int = 32,
        learning_rate_macro: float = 0.0001,
        learning_rate_micro: float = 0.0005,
        gamma: float = 0.99,
        gradient_accumulation_steps: int = 4,
        use_amp: bool = True,
        compile_model: bool = True,
        device: str = "cpu"
    ):
        self.device = torch.device(device)
        self.gamma = gamma
        self.num_envs = num_envs
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.use_amp = use_amp
        
        # Build policy network and choose dtype based on device
        policy = AsymmetricPolicyNetwork(
            macro_features=macro_features_dim,
            micro_features=micro_features_dim,
            macro_embedding_dim=256,
            micro_hidden_dim=256,
            num_actions=3
        )

        # Keep policy in float32 to ensure compatibility with MPS and with
        # the environment tensors (which are created as float32 by default).
        policy.float()
        policy = policy.to(self.device)
        
        # âœ… OTIMIZAÃ‡ÃƒO: Torch.compile (JIT) 
        # DESABILITADO: NÃ£o compatÃ­vel com Python 3.12+
        if compile_model and hasattr(torch, 'compile') and sys.version_info < (3, 12):
            try:
                print("ğŸš€ Compilando modelo com torch.compile...")
                self.policy = torch.compile(policy, backend="aot_eager")
                print("âœ… Modelo compilado com sucesso!")
            except Exception as e:
                print(f"âš ï¸  Torch.compile falhou: {e}")
                print("   Usando modelo sem compilaÃ§Ã£o.")
                self.policy = policy
        else:
            if compile_model and sys.version_info >= (3, 12):
                print("âš ï¸  Torch.compile nÃ£o suportado no Python 3.12+")
                print("   Usando modelo sem compilaÃ§Ã£o.")
            self.policy = policy
        
        # Temporarily set to eval for estabilidade (evitar bug BatchNorm no macOS)
        self.policy.eval()

        # Optimizers separados
        self.optimizer_macro = optim.Adam(
            list(self.policy.macro_encoder.parameters()) +
            list(self.policy.macro_bottleneck.parameters()) +
            list(self.policy.macro_decoder.parameters()),
            lr=learning_rate_macro
        )
        
        self.optimizer_micro = optim.Adam(
            list(self.policy.micro_processor.parameters()) +
            list(self.policy.decision_head.parameters()),
            lr=learning_rate_micro
        )
        
        # âœ… OTIMIZAÃ‡ÃƒO: Mixed Precision
        if self.use_amp and self.device.type == "mps":
            self.scaler_macro = GradScaler()
            self.scaler_micro = GradScaler()
            print("âœ… Mixed precision habilitado (AMP)")
        else:
            self.scaler_macro = None
            self.scaler_micro = None
        
        # HistÃ³rico
        self.history = {
            'episode': [],
            'total_reward': [],
            'portfolio_value': [],
            'macro_updates': [],
            'micro_updates': [],
            'episode_lengths': []
        }
        
        self.macro_update_count = 0
        self.micro_update_count = 0
        self._debug_call_count = 0
        
        logger.info(
            "optimized_trainer_initialized",
            num_envs=num_envs,
            macro_lr=learning_rate_macro,
            micro_lr=learning_rate_micro,
            device=device,
            amp=use_amp
        )
    
    def select_actions_batch(self, states: dict) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Select actions for batch of states (vectorized)
        
        Args:
            states: dict with (num_envs, ...) tensors
        
        Returns:
            actions: (num_envs,)
            log_probs: (num_envs,)
        """
        try:
            macro_feat = states['macro_features'].to(self.device)
            micro_feat = states['micro_features'].to(self.device)
            position = states['position'].to(self.device)
            cash_ratio = (states['cash'] / 10000.0).to(self.device)
        except Exception as e:
            print(f"âŒ Erro ao mover estados para device: {e}")
            print(f"   macro type: {type(states['macro_features'])}")
            raise

        macro_feat = macro_feat.contiguous().clone()
        micro_feat = micro_feat.contiguous().clone()

        if torch.isnan(macro_feat).any() or torch.isinf(macro_feat).any():
            print("âš ï¸  macro_feat contÃ©m NaN/Inf â€” corrigindo")
            macro_feat = torch.nan_to_num(macro_feat, nan=0.0, posinf=0.0, neginf=0.0)
        if torch.isnan(micro_feat).any() or torch.isinf(micro_feat).any():
            print("âš ï¸  micro_feat contÃ©m NaN/Inf â€” corrigindo")
            micro_feat = torch.nan_to_num(micro_feat, nan=0.0, posinf=0.0, neginf=0.0)

        # Clamp valores extremos para estabilizar BatchNorm
        macro_feat = torch.clamp(macro_feat, min=-1e4, max=1e4)
        micro_feat = torch.clamp(micro_feat, min=-1e4, max=1e4)
        
        # Forward pass with AMP
        self._debug_call_count += 1
        if self._debug_call_count <= 3:
            print(f"     â–¶ï¸ Forward call #{self._debug_call_count}: macro_feat {macro_feat.shape} (min {macro_feat.min().item():.2f}, max {macro_feat.max().item():.2f}), micro_feat max {micro_feat.max().item():.2f}")
        if self.use_amp and self.device.type == "mps":
            with autocast(device_type='mps'):
                action_probs, _ = self.policy(macro_feat, micro_feat, position, cash_ratio)
        else:
            action_probs, _ = self.policy(macro_feat, micro_feat, position, cash_ratio)

        if torch.isnan(action_probs).any() or torch.isinf(action_probs).any():
            print("âš ï¸  action_probs contÃ©m NaN/Inf â€” aplicando correÃ§Ãµes")
            action_probs = torch.nan_to_num(action_probs, nan=1.0/ action_probs.shape[-1], posinf=1.0/ action_probs.shape[-1], neginf=1.0/ action_probs.shape[-1])
        
        # Garantir distribuiÃ§Ã£o vÃ¡lida
        action_probs = torch.clamp(action_probs, min=1e-6)
        action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)
        
        # Sample actions (vectorized)
        try:
            dist = Categorical(action_probs)
            actions = dist.sample()
            log_probs = dist.log_prob(actions)
        except Exception as e:
            print(f"âš ï¸  Falha no Categorical: {e}. Usando fallback determinÃ­stico.")
            actions = torch.argmax(action_probs, dim=-1)
            log_probs = torch.log(torch.gather(action_probs, 1, actions.unsqueeze(-1)).squeeze(-1) + 1e-8)
        
        return actions, log_probs
    
    def compute_returns_vectorized(self, rewards: torch.Tensor) -> torch.Tensor:
        """
        Compute discounted returns (vectorized, sem loops!)
        
        Args:
            rewards: (T,) tensor
        
        Returns:
            returns: (T,) tensor
        """
        T = rewards.shape[0]
        returns = torch.zeros_like(rewards)
        
        # Vectorized computation using matrix multiplication
        gamma_matrix = torch.zeros((T, T), device=self.device)
        for i in range(T):
            for j in range(i, T):
                gamma_matrix[i, j] = self.gamma ** (j - i)
        
        returns = torch.matmul(gamma_matrix, rewards)
        
        return returns
    
    def train_batch(
        self,
        vec_env: VectorizedTradingEnv,
        update_macro: bool,
        update_micro: bool,
        max_steps: int = 1000
    ) -> Tuple[float, float]:
        """
        Train on batch of episodes (VECTORIZED)
        
        Args:
            vec_env: Vectorized environment
            update_macro: Update MacroNet?
            update_micro: Update MicroNet?
            max_steps: Max steps per episode
        
        Returns:
            mean_reward, mean_portfolio
        """
        states = vec_env.reset()
        
        # Collect trajectories
        all_log_probs = []
        all_rewards = []
        
        print("   ğŸ” Iniciando coleta de trajetÃ³rias...")
        for step in range(max_steps):
            # Select actions for all envs
            actions, log_probs = self.select_actions_batch(states)
            if step < 3:
                with torch.no_grad():
                    print(f"     Step {step}: aÃ§Ãµes {actions.cpu().numpy()} | log_probs min {log_probs.min().item():.4f} max {log_probs.max().item():.4f}")
            
            # Step all envs
            next_states, rewards, dones = vec_env.step(actions.cpu())
            if step < 3:
                print(f"     Step {step}: reward min {rewards.min().item():.4f} max {rewards.max().item():.4f}")
            
            # Store (only for non-done envs)
            all_log_probs.append(log_probs)
            all_rewards.append(rewards.to(self.device))
            
            states = next_states
            
            if dones.all():
                print(f"   âœ… Todos ambientes finalizados em {step+1} passos")
                break
        print("   ğŸ” Coleta finalizada, pilhando tensores...")
        
        # Stack trajectories
        log_probs_tensor = torch.stack(all_log_probs)  # (T, num_envs)
        rewards_tensor = torch.stack(all_rewards)      # (T, num_envs)
        print(f"   ğŸ“¦ TrajetÃ³rias empilhadas: log_probs {log_probs_tensor.shape}, rewards {rewards_tensor.shape}")
        
        # Compute returns for each env (vectorized)
        returns_list = []
        for env_idx in range(self.num_envs):
            env_rewards = rewards_tensor[:, env_idx]
            env_returns = self.compute_returns_vectorized(env_rewards)
            returns_list.append(env_returns)
        print("   âœ… Retornos computados para todos os ambientes")
        
        returns_tensor = torch.stack(returns_list, dim=1)  # (T, num_envs)
        
        # Flatten for loss computation
        log_probs_flat = log_probs_tensor.reshape(-1)
        returns_flat = returns_tensor.reshape(-1)
        
        # Normalize returns
        if returns_flat.numel() > 1:
            returns_flat = (returns_flat - returns_flat.mean()) / (returns_flat.std() + 1e-8)
        
        # âœ… OTIMIZAÃ‡ÃƒO: Vectorized policy loss
        policy_loss = -(log_probs_flat * returns_flat).mean()
        
        # Selective backpropagation with AMP
        if update_macro:
            if self.use_amp and self.scaler_macro:
                self.optimizer_macro.zero_grad()
                self.scaler_macro.scale(policy_loss).backward(retain_graph=update_micro)
                self.scaler_macro.unscale_(self.optimizer_macro)
                torch.nn.utils.clip_grad_norm_(
                    list(self.policy.macro_encoder.parameters()) +
                    list(self.policy.macro_bottleneck.parameters()) +
                    list(self.policy.macro_decoder.parameters()),
                    1.0
                )
                self.scaler_macro.step(self.optimizer_macro)
                self.scaler_macro.update()
            else:
                self.optimizer_macro.zero_grad()
                policy_loss.backward(retain_graph=update_micro)
                torch.nn.utils.clip_grad_norm_(
                    list(self.policy.macro_encoder.parameters()) +
                    list(self.policy.macro_bottleneck.parameters()) +
                    list(self.policy.macro_decoder.parameters()),
                    1.0
                )
                self.optimizer_macro.step()
            
            self.macro_update_count += 1
        
        if update_micro:
            if self.use_amp and self.scaler_micro:
                if not update_macro:
                    self.optimizer_micro.zero_grad()
                    self.scaler_micro.scale(policy_loss).backward()
                
                self.scaler_micro.unscale_(self.optimizer_micro)
                torch.nn.utils.clip_grad_norm_(
                    list(self.policy.micro_processor.parameters()) +
                    list(self.policy.decision_head.parameters()),
                    1.0
                )
                self.scaler_micro.step(self.optimizer_micro)
                self.scaler_micro.update()
            else:
                if not update_macro:
                    self.optimizer_micro.zero_grad()
                    policy_loss.backward()
                
                torch.nn.utils.clip_grad_norm_(
                    list(self.policy.micro_processor.parameters()) +
                    list(self.policy.decision_head.parameters()),
                    1.0
                )
                self.optimizer_micro.step()
            
            self.micro_update_count += 1
        
        # Calculate metrics
        total_rewards = rewards_tensor.sum(dim=0)  # (num_envs,)
        mean_reward = total_rewards.mean().item()
        
        # Calculate final portfolio values
        final_prices = states['price']
        final_portfolios = states['cash'] + (states['position'] * final_prices)
        mean_portfolio = final_portfolios.mean().item()
        
        return mean_reward, mean_portfolio


def prepare_asymmetric_data(
    df: pd.DataFrame,
    macro_window: int = 492,
    micro_window: int = 60
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Preparar dados com janelas assimÃ©tricas"""
    
    print(f"\nğŸ“Š Preparando dados assimÃ©tricos...")
    print(f"   DataFrame shape: {df.shape}")
    print(f"   Macro window: {macro_window} candles ({macro_window*5/60:.1f}h)")
    print(f"   Micro window: {micro_window} candles ({micro_window*5/60:.1f}h)")
    
    # Verificar se temos dados suficientes
    if len(df) <= macro_window:
        raise ValueError(f"Dados insuficientes! DataFrame tem {len(df)} candles, mas macro_window precisa de {macro_window}")
    
    # Extrair features numÃ©ricas (excluir colunas nÃ£o-numÃ©ricas)
    numeric_cols = []
    for col in df.columns:
        if np.issubdtype(df[col].dtype, np.number):
            numeric_cols.append(col)
    
    print(f"   Colunas numÃ©ricas encontradas: {len(numeric_cols)}")
    
    if len(numeric_cols) == 0:
        raise ValueError("Nenhuma coluna numÃ©rica encontrada no DataFrame!")
    
    # Use float32 to be compatible with MPS while keeping good precision
    features = df[numeric_cols].fillna(0).values.astype(np.float32)
    prices = df['close'].values.astype(np.float32)
    
    print(f"   Features shape: {features.shape}")
    print(f"   Prices shape: {prices.shape}")
    
    # Criar janelas rolantes
    macro_features_list = []
    micro_features_list = []
    prices_list = []
    
    print(f"   Iniciando loop de {macro_window} atÃ© {len(df)} ({len(df) - macro_window} amostras)")
    
    for i in range(macro_window, len(df)):
        # Macro: agregaÃ§Ã£o de longo prazo
        macro_window_data = features[i-macro_window:i]
        
        if macro_window_data.size == 0:
            print(f"   ERRO: macro_window_data vazio no Ã­ndice {i}")
            continue
            
        macro_agg = np.concatenate([
            macro_window_data.mean(axis=0),
            macro_window_data.std(axis=0),
            macro_window_data[-1]
        ])
        
        # Micro: agregaÃ§Ã£o de curto prazo
        micro_start = max(0, i - micro_window)
        micro_window_data = features[micro_start:i]
        if len(micro_window_data) < micro_window:
            pad_size = micro_window - len(micro_window_data)
            micro_window_data = np.vstack([
                np.zeros((pad_size, features.shape[1])),
                micro_window_data
            ])
        
        micro_agg = np.concatenate([
            micro_window_data.mean(axis=0),
            micro_window_data.std(axis=0),
            micro_window_data[-1]
        ])
        
        macro_features_list.append(macro_agg)
        micro_features_list.append(micro_agg)
        prices_list.append(prices[i])
    
    macro_features = np.array(macro_features_list, dtype=np.float32)
    micro_features = np.array(micro_features_list, dtype=np.float32)
    prices_array = np.array(prices_list, dtype=np.float32)
    
    print(f"âœ… Dados preparados:")
    print(f"   Samples: {len(prices_array)}")
    
    # Verificar se os arrays tÃªm a forma esperada
    if macro_features.ndim == 0 or len(macro_features) == 0:
        raise ValueError(f"Macro features vazio! macro_features.shape: {macro_features.shape}")
    if micro_features.ndim == 0 or len(micro_features) == 0:
        raise ValueError(f"Micro features vazio! micro_features.shape: {micro_features.shape}")
    
    print(f"   Macro features: {macro_features.shape[1] if macro_features.ndim > 1 else 'N/A (1D array)'}")
    print(f"   Micro features: {micro_features.shape[1] if micro_features.ndim > 1 else 'N/A (1D array)'}")
    print(f"   Macro features shape: {macro_features.shape}")
    print(f"   Micro features shape: {micro_features.shape}")
    
    return prices_array, macro_features, micro_features


def train_optimized_asymmetric_rl(
    duration_minutes: int = 10,
    log_interval_seconds: int = 30,
    num_envs: int = 64
):
    """
    Treinar com TODAS as otimizaÃ§Ãµes para M2
    """
    print("\n" + "="*70)
    print("  ğŸš€ TREINAMENTO OTIMIZADO PARA APPLE M2")
    print("  âœ… Batch Processing (32 envs paralelos)")
    print("  âœ… Torch.compile (JIT compilation)")
    print("  âœ… OperaÃ§Ãµes vetorizadas (sem loops)")
    print("  âœ… Usando float32 para compatibilidade com MPS (AMP desabilitado por padrÃ£o)")
    print("  ")
    print("  âš ï¸  Mixed Precision DESABILITADO")
    print("      Trading requer precisÃ£o numÃ©rica!")
    print("  ")
    print("  MacroNet: 1x update (estratÃ©gia)")
    print("  MicroNet: 10x updates (tÃ¡tica)")
    print("  Ratio: 1:10 ğŸ¯")
    print("="*70 + "\n")
    
    # 1. Carregar dados de 2024
    print("ğŸ“… Carregando dados de 2024 a partir do parquet local...")
    from src.features.builder import FeatureBuilder

    data_path = Path("data/timeframe=5m/symbol=BTCUSDT/candles.parquet")
    if not data_path.exists():
        raise FileNotFoundError(
            f"Arquivo de candles nÃ£o encontrado em {data_path.resolve()}"
        )

    df_all = pd.read_parquet(data_path, engine="pyarrow")
    if 'timestamp' not in df_all.columns:
        raise KeyError("Coluna 'timestamp' ausente no parquet de candles")

    df_all = df_all.sort_values('timestamp').reset_index(drop=True)

    # Normalizar colunas principais que podem chegar como string
    base_numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'quote_volume']
    for col in base_numeric_cols:
        if col in df_all.columns:
            df_all[col] = pd.to_numeric(df_all[col], errors='coerce')

    print(f"ğŸ“Š df_all shape: {df_all.shape}")
    if df_all.empty:
        raise ValueError("Parquet de candles estÃ¡ vazio")
    
    print(f"   Timestamp range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}")
    print(f"   Columns: {list(df_all.columns)}")

    # Try to get 2024 data first
    df_2024 = df_all[
        (df_all['timestamp'] >= datetime(2024, 1, 1)) &
        (df_all['timestamp'] < datetime(2025, 1, 1))
    ].copy()
    
    min_samples_needed = 600  # macro_window(492) + buffer
    
    if len(df_2024) >= min_samples_needed:
        # Perfect! We have enough 2024 data
        df_train = df_2024.copy()
        print(f"âœ… Usando {len(df_train)} candles de 2024 para treinamento")
    elif len(df_all) >= min_samples_needed:
        # Use the oldest available data (for more historical context)
        df_train = df_all.head(min_samples_needed).copy()
        print(f"âš ï¸  Usando {len(df_train)} candles mais antigos disponÃ­veis para treinamento")
        print(f"   (Sem dados suficientes de 2024: {len(df_2024)} candles)")
    else:
        raise ValueError(
            f"Dados insuficientes no parquet ({len(df_all)} < {min_samples_needed}) para preparar macro window"
        )
    
    print(f"   Training data range: {df_train['timestamp'].min()} to {df_train['timestamp'].max()}")

    builder = FeatureBuilder()
    df_train = builder.build_features(df_train)

    print(f"âœ… {len(df_train)} candles preparados (apÃ³s build_features)")
    print(f"   Final columns: {list(df_train.columns) if not df_train.empty else 'DataFrame vazio!'}")
    
    if df_train.empty:
        print("âŒ ERRO: DataFrame vazio apÃ³s build_features!")
        return
    
    # 2. Preparar dados
    prices, macro_features, micro_features = prepare_asymmetric_data(
        df_train,
        macro_window=492,
        micro_window=60
    )
    
    # 3. Criar ambiente vetorizado
    print(f"\nğŸ® Criando {num_envs} ambientes paralelos...")
    try:
        vec_env = VectorizedTradingEnv(
            prices=prices,
            macro_features=macro_features,
            micro_features=micro_features,
            num_envs=num_envs,
            initial_capital=10000.0,
            commission=0.001,
            device="cpu"  # Env fica em CPU, policy em GPU
        )
        print("âœ… Ambientes criados com sucesso!")
    except Exception as e:
        print(f"âŒ ERRO ao criar ambientes: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. Criar trainer otimizado
    print(f"\nğŸ§  Criando trainer otimizado...")
    print(f"   Macro features dim: {macro_features.shape[1]}")
    print(f"   Micro features dim: {micro_features.shape[1]}")
    try:
        trainer = OptimizedAsymmetricTrainer(
            macro_features_dim=macro_features.shape[1],
            micro_features_dim=micro_features.shape[1],
            num_envs=num_envs,
            learning_rate_macro=0.0001,
            learning_rate_micro=0.0005,
            gamma=0.99,
            gradient_accumulation_steps=4,
            use_amp=False,  # DESABILITADO: Manter float64 para precisÃ£o em trading
            compile_model=True,
            device="cpu"  # ForÃ§ar CPU para evitar problemas com MPS
        )
        print("âœ… Trainer criado com sucesso!")
    except Exception as e:
        print(f"âŒ ERRO ao criar trainer: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print(f"\nğŸš€ Iniciando treinamento por {duration_minutes} minutos...")
    print(f"ğŸ“Š Dataset: {len(prices)} candles")
    print(f"ğŸ’° Capital inicial: $10,000")
    print(f"ğŸ® Ambientes paralelos: {num_envs}")
    # ForÃ§ar CPU para evitar problemas com MPS
    config.device = "cpu"
    print(f"âš™ï¸  Device: {config.device} (forÃ§ado para CPU)")
    print(f"â° Log a cada {log_interval_seconds}s\n")
    
    # HistÃ³rico
    history = {
        'time_min': [],
        'batch': [],
        'avg_reward': [],
        'avg_portfolio': [],
        'macro_updates': [],
        'micro_updates': [],
        'batches_per_sec': []
    }
    
    start_time = time.time()
    end_time = start_time + (duration_minutes * 60)
    last_log_time = start_time
    
    batch = 0
    recent_rewards = deque(maxlen=20)
    recent_portfolios = deque(maxlen=20)
    batches_since_log = 0
    
    while time.time() < end_time:
        # Decidir quais componentes atualizar
        cycle_position = batch % 11
        
        if cycle_position == 0:
            update_macro = True
            update_micro = True
        else:
            update_macro = False
            update_micro = True
        
        # Treinar batch
        mean_reward, mean_portfolio = trainer.train_batch(
            vec_env,
            update_macro=update_macro,
            update_micro=update_micro,
            max_steps=500  # Reduzido para convergir mais rÃ¡pido
        )
        
        batch += 1
        batches_since_log += 1
        recent_rewards.append(mean_reward)
        recent_portfolios.append(mean_portfolio)
        
        # Log periÃ³dico
        current_time = time.time()
        if current_time - last_log_time >= log_interval_seconds:
            elapsed = current_time - start_time
            remaining = end_time - current_time
            progress = (elapsed / (duration_minutes * 60)) * 100
            
            avg_reward = np.mean(recent_rewards) if recent_rewards else 0
            avg_portfolio = np.mean(recent_portfolios) if recent_portfolios else 10000
            avg_return_pct = ((avg_portfolio - 10000) / 10000) * 100
            
            batches_per_sec = batches_since_log / log_interval_seconds
            episodes_per_sec = batches_per_sec * num_envs
            
            print(f"\n{'â”€'*70}")
            print(f"â±ï¸  Tempo: {elapsed/60:.1f}min / {duration_minutes}min ({progress:.1f}%)")
            print(f"ğŸ® Batch: {batch} ({batches_per_sec:.1f} batches/s = {episodes_per_sec:.0f} eps/s)")
            print(f"ğŸ”„ Updates - Macro: {trainer.macro_update_count} | Micro: {trainer.micro_update_count}")
            print(f"ğŸ“Š Ratio atual: 1:{trainer.micro_update_count/max(1,trainer.macro_update_count):.1f}")
            print(f"ğŸ’° Portfolio mÃ©dio: ${avg_portfolio:,.2f} ({avg_return_pct:+.2f}%)")
            print(f"ğŸ“ˆ Reward mÃ©dio: {avg_reward:.2f}")
            print(f"â³ Restante: {remaining/60:.1f}min")
            print(f"{'â”€'*70}")
            
            # Salvar histÃ³rico
            history['time_min'].append(elapsed / 60)
            history['batch'].append(batch)
            history['avg_reward'].append(avg_reward)
            history['avg_portfolio'].append(avg_portfolio)
            history['macro_updates'].append(trainer.macro_update_count)
            history['micro_updates'].append(trainer.micro_update_count)
            history['batches_per_sec'].append(batches_per_sec)
            
            last_log_time = current_time
            batches_since_log = 0
    
    # Final
    total_time = time.time() - start_time
    
    # Salvar modelo
    output_dir = Path("./training_results_optimized")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = output_dir / "optimized_policy.pt"
    torch.save(trainer.policy.state_dict(), model_path)
    
    # Plotar
    plot_optimized_history(history, output_dir, total_time, batch, trainer, num_envs)
    
    total_episodes = batch * num_envs
    eps_per_min = total_episodes / (total_time / 60)
    
    print(f"\nâœ… Treinamento otimizado completo!")
    print(f"â±ï¸  Tempo total: {total_time/60:.1f}min")
    print(f"ğŸ® Total de batches: {batch}")
    print(f"ğŸ¯ Total de episÃ³dios: {total_episodes} ({eps_per_min:.0f} eps/min)")
    print(f"ğŸ”„ Updates - Macro: {trainer.macro_update_count} | Micro: {trainer.micro_update_count}")
    print(f"ğŸ“Š Ratio final: 1:{trainer.micro_update_count/trainer.macro_update_count:.2f}")
    print(f"ğŸ’¾ Modelo salvo em: {output_dir}/")


def plot_optimized_history(
    history: dict,
    output_dir: Path,
    total_time: float,
    batches: int,
    trainer: OptimizedAsymmetricTrainer,
    num_envs: int
):
    """Plotar histÃ³rico otimizado"""
    
    if len(history['time_min']) == 0:
        print("âš ï¸  Sem dados para plotar")
        return
    
    print(f"\nğŸ“Š Gerando grÃ¡fico de evoluÃ§Ã£o...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle(
        f'Treinamento Otimizado M2 (1:10) - {total_time/60:.1f} min, {batches} batches, {num_envs} envs',
        fontsize=14,
        fontweight='bold'
    )
    
    time_axis = history['time_min']
    
    # 1. Portfolio
    ax1 = axes[0, 0]
    ax1.plot(time_axis, history['avg_portfolio'], 'b-', linewidth=2)
    ax1.axhline(y=10000, color='gray', linestyle='--', alpha=0.5, label='Initial')
    ax1.set_xlabel('Tempo (minutos)')
    ax1.set_ylabel('Portfolio Value ($)')
    ax1.set_title('EvoluÃ§Ã£o do Portfolio')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.1f}k'))
    
    # 2. Returns
    ax2 = axes[0, 1]
    returns = [((p - 10000) / 10000) * 100 for p in history['avg_portfolio']]
    ax2.plot(time_axis, returns, 'g-', linewidth=2)
    ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax2.set_xlabel('Tempo (minutos)')
    ax2.set_ylabel('Return (%)')
    ax2.set_title('Retorno Percentual')
    ax2.grid(True, alpha=0.3)
    
    # 3. Rewards
    ax3 = axes[0, 2]
    ax3.plot(time_axis, history['avg_reward'], 'orange', linewidth=2)
    ax3.set_xlabel('Tempo (minutos)')
    ax3.set_ylabel('Reward MÃ©dio')
    ax3.set_title('EvoluÃ§Ã£o dos Rewards')
    ax3.grid(True, alpha=0.3)
    
    # 4. Updates Count
    ax4 = axes[1, 0]
    ax4.plot(time_axis, history['macro_updates'], 'r-', linewidth=2, label='Macro Updates')
    ax4.plot(time_axis, history['micro_updates'], 'b-', linewidth=2, label='Micro Updates')
    ax4.set_xlabel('Tempo (minutos)')
    ax4.set_ylabel('NÃºmero de Updates')
    ax4.set_title('Updates por Componente')
    ax4.grid(True, alpha=0.3)
    ax4.legend()
    
    # 5. Update Ratio
    ax5 = axes[1, 1]
    ratios = [m / max(1, macro) for m, macro in zip(history['micro_updates'], history['macro_updates'])]
    ax5.plot(time_axis, ratios, 'purple', linewidth=2)
    ax5.axhline(y=10.0, color='orange', linestyle='--', alpha=0.5, label='Target Ratio (10:1)')
    ax5.set_xlabel('Tempo (minutos)')
    ax5.set_ylabel('Ratio (Micro:Macro)')
    ax5.set_title('Ratio de Updates')
    ax5.grid(True, alpha=0.3)
    ax5.legend()
    
    # 6. Throughput
    ax6 = axes[1, 2]
    ax6.plot(time_axis, history['batches_per_sec'], 'cyan', linewidth=2)
    ax6.set_xlabel('Tempo (minutos)')
    ax6.set_ylabel('Batches/segundo')
    ax6.set_title('Throughput (Performance)')
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    plot_path = output_dir / 'optimized_training_evolution.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ… GrÃ¡fico salvo: {plot_path}")
    
    plt.close()


if __name__ == "__main__":
    train_optimized_asymmetric_rl(
        duration_minutes=10,
        log_interval_seconds=30,
        num_envs=64  # Aumentado para melhor uso do M2
    )

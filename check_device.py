"""
Script para detectar e diagnosticar dispositivos de computa√ß√£o dispon√≠veis
Execut√°vel: python check_device.py
"""
import sys
sys.path.insert(0, '.')

def check_pytorch():
    """Verificar PyTorch e dispositivos dispon√≠veis"""
    print("\n" + "="*70)
    print("  PYTORCH - VERIFICA√á√ÉO DE DISPOSITIVOS")
    print("="*70 + "\n")
    
    try:
        import torch
        print(f"‚úì PyTorch vers√£o: {torch.__version__}")
        
        # CPU
        print(f"\nüìå CPU:")
        print(f"   ‚Ä¢ Dispon√≠vel: Sim")
        import multiprocessing
        print(f"   ‚Ä¢ Cores: {multiprocessing.cpu_count()}")
        
        # CUDA (NVIDIA GPU)
        print(f"\nüî∑ NVIDIA CUDA:")
        if torch.cuda.is_available():
            print(f"   ‚úì Dispon√≠vel: SIM")
            print(f"   ‚Ä¢ Vers√£o CUDA: {torch.version.cuda}")
            print(f"   ‚Ä¢ cuDNN vers√£o: {torch.backends.cudnn.version()}")
            print(f"   ‚Ä¢ GPUs detectadas: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"     - {i}: {torch.cuda.get_device_name(i)}")
        else:
            print(f"   ‚úó Dispon√≠vel: N√ÉO")
        
        # MPS (Apple Metal - M1/M2/M3)
        print(f"\nüçé APPLE METAL (M1/M2/M3/M4):")
        if hasattr(torch.backends, "mps"):
            if torch.backends.mps.is_available():
                print(f"   ‚úì Dispon√≠vel: SIM")
                print(f"   ‚Ä¢ Metal Performance Shaders habilitado")
            else:
                print(f"   ‚úó Dispon√≠vel: N√ÉO")
                if torch.backends.mps.is_built():
                    print(f"   ‚Ä¢ MPS constru√≠do, mas n√£o dispon√≠vel neste sistema")
        else:
            print(f"   ‚úó Dispon√≠vel: N√ÉO (PyTorch sem suporte MPS)")
        
        # XPU (Intel GPU - Arc, Iris Xe)
            print(f"\nüí† INTEL GPU (Arc, Iris Xe):")
            xpu_supported = False
            try:
                # Check intel extension for pytorch
                import importlib
                if importlib.util.find_spec('intel_extension_for_pytorch') is not None:
                    # ipex installed - report presence and recommend usage
                    print(f"   ‚úì ipex (intel-extension-for-pytorch) detectado")
                    xpu_supported = True
                else:
                    # fallback to torch.backends.xpu if available
                    if hasattr(torch.backends, 'xpu') and getattr(torch.backends.xpu, 'is_available', lambda: False)():
                        print(f"   ‚úì Dispon√≠vel: SIM (torch.backends.xpu)")
                        xpu_supported = True
                    else:
                        print(f"   ‚úó Dispon√≠vel: N√ÉO")
            except Exception:
                print(f"   ‚úó Erro ao checar XPU")

            if not xpu_supported:
                print(f"   ‚Ä¢ Para usar GPU Intel, instale: pip install intel-extension-for-pytorch")
                print(f"   ‚Ä¢ Depois, reinicie o interpretador/terminal (feche e reabra a sess√£o Python)")
        
    except ImportError:
        print("‚úó PyTorch n√£o instalado!")
        print("  Instale: pip install torch")


def check_cpu_capabilities():
    """Verificar capacidades da CPU"""
    print("\n" + "="*70)
    print("  CPU - CAPACIDADES E INSTRU√á√ïES")
    print("="*70 + "\n")
    
    try:
        import cpuinfo
        info = cpuinfo.get_cpu_info()
        
        print(f"Marca: {info.get('brand_raw', 'Desconhecida')}")
        print(f"Modelo: {info.get('hz_advertised_friendly', 'Desconhecido')}")
        
        # Flags de instru√ß√£o
        print(f"\nInstru√ß√µes suportadas:")
        flags = info.get('flags', [])
        
        # Agrupar flags importantes
        simd_flags = [f for f in flags if any(x in f for x in ['sse', 'avx', 'neon', 'sve'])]
        if simd_flags:
            print(f"   ‚Ä¢ SIMD: {', '.join(simd_flags)}")
        
        gpu_flags = [f for f in flags if any(x in f for x in ['gpu', 'igpu'])]
        if gpu_flags:
            print(f"   ‚Ä¢ GPU integrada: {', '.join(gpu_flags)}")
        
        ai_flags = [f for f in flags if any(x in f for x in ['vnni', 'amx', 'bf16'])]
        if ai_flags:
            print(f"   ‚Ä¢ AI acelera√ß√£o: {', '.join(ai_flags)}")
        
    except ImportError:
        print("‚ÑπÔ∏è  cpuinfo n√£o instalado")
        print("   Instale: pip install py-cpuinfo")


def check_current_config():
    """Verificar configura√ß√£o atual do projeto"""
    print("\n" + "="*70)
    print("  PROJETO - CONFIGURA√á√ÉO ATUAL")
    print("="*70 + "\n")
    
    from src.config import config
    
    print(f"üìå Device detectado: {config.device.upper()}")
    
    # Dar recomenda√ß√µes
    print(f"\nüí° Recomenda√ß√µes:")
    
    if config.device == "cuda":
        print(f"   ‚úì GPU NVIDIA detectada - Performance M√ÅXIMA")
        print(f"   ‚Ä¢ Modelos rodando em paralelo na GPU")
        print(f"   ‚Ä¢ Treino ~10-20x mais r√°pido que CPU")
    
    elif config.device == "mps":
        print(f"   ‚úì Apple Metal detectado - Performance ALTA")
        print(f"   ‚Ä¢ Otimizado para M1/M2/M3/M4")
        print(f"   ‚Ä¢ Treino ~5-10x mais r√°pido que CPU")
        print(f"   ‚Ä¢ Melhor que CPU integrada Intel")
    
    elif config.device == "xpu":
        print(f"   ‚úì Intel GPU detectada - Performance √ìTIMA")
        print(f"   ‚Ä¢ GPU Arc ou Iris Xe habilitada")
        print(f"   ‚Ä¢ Treino ~8-15x mais r√°pido que CPU")
    
    else:  # CPU
        print(f"   ‚ö†Ô∏è  CPU detectada - Performance B√ÅSICA")
        print(f"   ‚Ä¢ Usando apenas CPU")
        print(f"   ‚Ä¢ Treino mais lento, mas funciona")
        print(f"   ‚Ä¢ Recomenda√ß√µes:")
        print(f"     - Se tem NVIDIA: instale CUDA Toolkit")
        print(f"     - Se tem Intel Arc: pip install intel-extension-for-pytorch")
        print(f"     - Se tem M1/M2/M3: MPS j√° ativado via PyTorch")


def compare_performance():
    """Comparar performance em diferentes dispositivos"""
    print("\n" + "="*70)
    print("  COMPARA√á√ÉO DE PERFORMANCE (Tempo estimado por epoch)")
    print("="*70 + "\n")
    
    data = {
        "CPU": "~10-30 segundos",
        "CPU Intel (VNNI)": "~5-10 segundos",
        "Apple M1/M2/M3": "~2-5 segundos",
        "Intel GPU (Arc)": "~1-3 segundos",
        "NVIDIA RTX 4080": "~0.5-1 segundo"
    }
    
    print("Treino de MacroNet (1440 candles √ó 13 features):\n")
    for device, time_est in data.items():
        print(f"  ‚Ä¢ {device:.<30} {time_est}")
    
    print("\nüìä Backtest (30 dias de trading):\n")
    
    data_backtest = {
        "CPU": "~2-5 minutos",
        "CPU Intel (VNNI)": "~1-2 minutos",
        "Apple M1/M2/M3": "~30-60 segundos",
        "Intel GPU (Arc)": "~10-30 segundos",
        "NVIDIA RTX 4080": "~5-10 segundos"
    }
    
    for device, time_est in data_backtest.items():
        print(f"  ‚Ä¢ {device:.<30} {time_est}")


def main():
    """Executar diagn√≥stico completo"""
    print("\n" + "‚ñà"*70)
    print("‚ñà" + " "*68 + "‚ñà")
    print("‚ñà" + "  üîß DIAGN√ìSTICO DE HARDWARE - CPPNCRIPTO".center(68) + "‚ñà")
    print("‚ñà" + " "*68 + "‚ñà")
    print("‚ñà"*70)
    
    check_pytorch()
    check_cpu_capabilities()
    check_current_config()
    compare_performance()
    
    print("\n" + "="*70)
    print("  INSTRU√á√ïES DE INSTALA√á√ÉO")
    print("="*70 + "\n")
    
    print("Para usar GPU NVIDIA (CUDA):")
    print("  1. Instale CUDA Toolkit: https://developer.nvidia.com/cuda-downloads")
    print("  2. Instale cuDNN: https://developer.nvidia.com/cudnn")
    print("  3. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\nPara usar GPU Intel (Arc/Iris Xe):")
    print("  1. Instale Intel Extension for PyTorch")
    print("  2. pip install intel-extension-for-pytorch")
    print("  3. Reinicie o kernel/terminal Python (feche e reabra o interpretador) ")
    
    print("\nPara usar Apple Metal (M1/M2/M3):")
    print("  1. PyTorch j√° tem suporte nativo via MPS")
    print("  2. Deve funcionar automaticamente no Apple Silicon")
    
    print("\n‚úÖ Diagn√≥stico completo!\n")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
